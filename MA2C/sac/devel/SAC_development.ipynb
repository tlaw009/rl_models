{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc7ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import regularizers\n",
    "import glfw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cca3c",
   "metadata": {},
   "source": [
    "### Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e91a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"Hopper-v3\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafe2ab",
   "metadata": {},
   "source": [
    "### Actor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aeb90",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279be0e3",
   "metadata": {},
   "source": [
    "Same as PPO actor. Gaussian policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10\n",
    "\n",
    "class Actor(Model):\n",
    "\n",
    "    def __init__(self, action_dimensions, action_bound):\n",
    "        super().__init__()\n",
    "        self.action_dim, self.upper_bound = action_dimensions, action_bound\n",
    "        self.sample_dist = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(self.action_dim),\n",
    "                                                                    scale_diag=tf.ones(self.action_dim))\n",
    "        self.dense1_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.mean_layer = layers.Dense(self.action_dim)\n",
    "        self.stdev_layer = layers.Dense(self.action_dim)\n",
    "\n",
    "    def call(self, state, eval_mode=False):\n",
    "\n",
    "        a1 = self.dense1_layer(state)\n",
    "        a2 = self.dense2_layer(a1)\n",
    "        mu = self.mean_layer(a2)\n",
    "\n",
    "        log_sigma = self.stdev_layer(a2)\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        sigma = tf.clip_by_value(sigma, EPSILON, 2.718)\n",
    "\n",
    "        dist = tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "        \n",
    "        if eval_mode:\n",
    "            action_ = mu\n",
    "        else:\n",
    "            action_ = tf.math.add(mu, tf.math.multiply(sigma, tf.expand_dims(self.sample_dist.sample(), 0)))\n",
    " \n",
    "        action = tf.tanh(action_)\n",
    "\n",
    "        log_pi_ = dist.log_prob(action_)     \n",
    "        log_pi = log_pi_ - tf.reduce_sum(tf.math.log(tf.clip_by_value(1 - action**2, EPSILON, 1.0)), axis=1)\n",
    "        \n",
    "        return action*self.upper_bound, log_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3f74b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 18:11:31.393652: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.409017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.409454: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.411937: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-30 18:11:31.413635: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.414067: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.414425: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.723463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.723588: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.723673: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-30 18:11:31.723747: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5626 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "actor_test = Actor(num_actions, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7cab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.8582691  0.60904384 0.00860028]], shape=(1, 3), dtype=float32) tf.Tensor([-1.4346583], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-30 18:11:32.214426: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs\n",
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "tf_obs\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "print(a_test, log_a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116539f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               multiple                  768       \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  195       \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,318\n",
      "Trainable params: 5,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fae85",
   "metadata": {},
   "source": [
    "### Critic Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a403d18b",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347d502",
   "metadata": {},
   "source": [
    "Different from PPO, critic evaluate state-action pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Wrapper():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.s_dim=state_dim\n",
    "        self.a_dim=action_dim\n",
    "        \n",
    "    def get_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(self.s_dim))\n",
    "        state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(self.a_dim))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "        out = layers.Dense(64, activation=\"relu\")(concat)\n",
    "        outputs = tf.squeeze(layers.Dense(1)(out))\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dadd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_gen = Critic_Wrapper(num_states, num_actions)\n",
    "critic_test = critic_gen.get_critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65aced4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25289728e+00, -1.24963452e-03, -4.69490706e-03,  2.35503294e-03,\n",
       "       -1.52612720e-03,  2.06331169e-04, -3.52340801e-03, -6.81939228e-04,\n",
       "        4.28433440e-03,  1.21088773e-03,  4.06597211e-03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c101830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11), dtype=float64, numpy=\n",
       "array([[ 1.25289728e+00, -1.24963452e-03, -4.69490706e-03,\n",
       "         2.35503294e-03, -1.52612720e-03,  2.06331169e-04,\n",
       "        -3.52340801e-03, -6.81939228e-04,  4.28433440e-03,\n",
       "         1.21088773e-03,  4.06597211e-03]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "tf_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa334304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.16794941>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_test = critic_test([tf_obs, a_test])\n",
    "v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a2cfe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11)\n"
     ]
    }
   ],
   "source": [
    "obs_new, _, _, _ = env.step(a_test[0])\n",
    "tf_obs_new = tf.expand_dims(obs_new, 0)\n",
    "statex2 = tf.convert_to_tensor([obs, obs_new])\n",
    "print(statex2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df584535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2,)\n"
     ]
    }
   ],
   "source": [
    "a_2, loga_2 = actor_test(statex2)\n",
    "\n",
    "print(a_2.shape, loga_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f16a7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "v_2 = critic_test([statex2, a_2])\n",
    "print(v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb6392ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           384         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           128         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64)           0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           4160        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            65          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  None                0           ['dense_7[0][0]']                \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3c971",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66ff1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, obs_dim, a_dim, buffer_capacity=100000, batch_size=256):\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, self.a_dim))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "        self.buffer_counter += 1\n",
    "        \n",
    "    def sample(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.squeeze(tf.convert_to_tensor(self.reward_buffer[batch_indices]))\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.squeeze(tf.convert_to_tensor(self.done_buffer[batch_indices]))\n",
    "        \n",
    "        return (state_batch,\n",
    "               action_batch,\n",
    "               reward_batch,\n",
    "               next_state_batch,\n",
    "               done_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d040f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer1 = Buffer(num_states, num_actions, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11879892",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_obs = env.reset()\n",
    "\n",
    "for i in range(buffer1.buffer_capacity):\n",
    "    a, _ = actor_test(tf.expand_dims(prev_obs, 0))\n",
    "    obs, r, d, _ = env.step(a[0])\n",
    "    \n",
    "    buffer1.record((prev_obs, a[0], r, obs, d))\n",
    "    \n",
    "    prev_obs = obs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac7dac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 1.18538629e+00, -2.43833702e-01,  1.64513053e-03,\n",
       "         -4.31658168e-01,  1.54749131e-01, -5.63677487e-01,\n",
       "         -4.55972140e-01, -2.16385357e+00, -1.97891503e-02,\n",
       "         -3.75202752e+00,  2.04522712e+00],\n",
       "        [ 1.05683490e+00, -6.10759501e-01,  9.50808670e-03,\n",
       "         -9.71017115e-01,  3.58387415e-01, -3.97285040e-01,\n",
       "         -4.43918546e-01, -1.15316134e+00, -7.75623207e-02,\n",
       "         -1.63858226e+00,  1.49019619e+00],\n",
       "        [ 1.14626738e+00, -3.59140357e-01,  1.93936090e-03,\n",
       "         -6.30134803e-01,  2.76810101e-01, -3.90021481e-01,\n",
       "         -4.24060350e-01, -1.15520027e+00, -1.74236089e-02,\n",
       "         -1.74144622e+00,  5.39387238e-01],\n",
       "        [ 1.24455034e+00, -5.48225928e-03,  2.68298116e-03,\n",
       "         -5.37246848e-03, -5.24586539e-06, -7.42097135e-02,\n",
       "         -2.55240233e-01, -3.27880187e-01,  2.43391650e-01,\n",
       "         -1.23259300e+00,  1.51508172e+00],\n",
       "        [ 1.07632367e+00, -5.61102814e-01,  1.51116196e-03,\n",
       "         -9.14156691e-01,  3.87205842e-01, -3.57573243e-01,\n",
       "         -2.59424199e-01, -5.39852221e-01,  4.73888385e-02,\n",
       "         -3.86468175e-01, -1.23027194e+00],\n",
       "        [ 1.16082842e+00, -3.19341242e-01,  1.82555294e-03,\n",
       "         -5.63596525e-01,  2.53620778e-01, -5.09751750e-01,\n",
       "         -7.17589088e-01, -1.94693148e+00,  6.59575694e-03,\n",
       "         -3.43608366e+00,  1.97941373e+00],\n",
       "        [ 1.22007181e+00, -1.21342170e-02,  4.12936409e-03,\n",
       "         -2.82803048e-02,  1.07453220e-01,  3.34376894e-04,\n",
       "         -6.32022598e-01, -2.44418475e-01, -2.20109320e-02,\n",
       "         -6.21990526e-01,  1.46485528e+00],\n",
       "        [ 1.21102499e+00, -9.00191513e-03,  2.08594408e-03,\n",
       "         -7.00038256e-03, -1.08801767e-02, -2.89734857e-01,\n",
       "         -1.21136440e-01, -3.77106918e-01, -9.19660334e-03,\n",
       "         -2.85999604e-01,  5.94558896e-01],\n",
       "        [ 1.20362494e+00, -8.33231956e-02,  1.01834436e-03,\n",
       "         -1.40209009e-01,  8.44909248e-02, -3.02924008e-01,\n",
       "         -1.26521090e-01, -1.46898154e+00,  4.94879681e-01,\n",
       "         -3.75786952e+00,  1.53513814e+00],\n",
       "        [ 1.21232963e+00, -4.05466256e-04,  2.15960610e-03,\n",
       "          3.71407228e-03, -3.20504190e-02, -2.19905724e-01,\n",
       "         -3.93937892e-02, -2.22266352e-01, -1.01997480e-02,\n",
       "         -6.79645790e-02,  1.87650846e+00]])>,\n",
       " <tf.Tensor: shape=(10, 3), dtype=float64, numpy=\n",
       " array([[ 0.93462795, -0.03505964,  0.36812553],\n",
       "        [ 0.94266605,  0.58067632, -0.727238  ],\n",
       "        [ 0.96103764, -0.69765508,  0.54208493],\n",
       "        [ 0.8267262 ,  0.74331349,  0.98391926],\n",
       "        [ 0.66304052,  0.74311638,  0.12229487],\n",
       "        [ 0.54398417,  0.12080994, -0.43218786],\n",
       "        [ 0.86358368, -0.3598235 ,  0.60581952],\n",
       "        [ 0.80322832, -0.69215113,  0.9409427 ],\n",
       "        [ 0.700014  , -0.48246422,  0.43206957],\n",
       "        [ 0.60123748, -0.85655177, -0.38607514]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       " array([0.44230719, 0.60677785, 0.58255489, 0.97695538, 0.68418943,\n",
       "        0.49853107, 1.01265654, 0.6809366 , 0.66882119, 0.716811  ])>,\n",
       " <tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 1.18131769e+00, -2.60961685e-01,  1.64950845e-03,\n",
       "         -4.61842958e-01,  1.73200195e-01, -5.50133791e-01,\n",
       "         -5.61974332e-01, -2.12392899e+00,  1.61245557e-02,\n",
       "         -3.79261555e+00,  2.56575294e+00],\n",
       "        [ 1.05366539e+00, -6.19130400e-01,  8.70779213e-03,\n",
       "         -9.81555496e-01,  3.65861082e-01, -3.90057045e-01,\n",
       "         -3.43119592e-01, -9.35036893e-01, -1.14884676e-01,\n",
       "         -9.95788629e-01,  4.06090166e-01],\n",
       "        [ 1.14241273e+00, -3.70106144e-01,  1.95532108e-03,\n",
       "         -6.48098504e-01,  2.84032182e-01, -4.40425591e-01,\n",
       "         -5.44239044e-01, -1.58891128e+00,  1.68288490e-02,\n",
       "         -2.74590521e+00,  1.25943260e+00],\n",
       "        [ 1.24203971e+00, -6.70280041e-03,  3.99501839e-03,\n",
       "         -1.14060862e-02,  1.75883381e-02,  3.29751672e-02,\n",
       "         -3.72034173e-01,  3.37146339e-02,  9.93979900e-02,\n",
       "         -2.77645271e-01,  2.88160248e+00],\n",
       "        [ 1.07479443e+00, -5.63900502e-01,  1.80047731e-03,\n",
       "         -9.14509752e-01,  3.77166708e-01, -2.71500237e-01,\n",
       "         -1.20405830e-01, -1.59300024e-01,  2.66781841e-02,\n",
       "          2.91293260e-01, -1.28481009e+00],\n",
       "        [ 1.15510383e+00, -3.34802116e-01,  1.86759357e-03,\n",
       "         -5.90723504e-01,  2.66357777e-01, -4.99669080e-01,\n",
       "         -7.05218130e-01, -1.92122563e+00,  4.23915087e-03,\n",
       "         -3.33335297e+00,  1.25516175e+00],\n",
       "        [ 1.21567747e+00, -1.50812874e-02,  3.95255828e-03,\n",
       "         -3.61539900e-02,  1.20886340e-01,  2.62097894e-02,\n",
       "         -4.68611033e-01, -4.95505235e-01, -2.16104627e-02,\n",
       "         -1.34252473e+00,  1.89688187e+00],\n",
       "        [ 1.20981808e+00, -1.36543664e-02,  2.09994629e-03,\n",
       "         -1.33918600e-02, -7.73758220e-04, -3.37513985e-01,\n",
       "         -1.45970890e-01, -7.94892323e-01,  9.89872192e-03,\n",
       "         -1.33552738e+00,  1.86715210e+00],\n",
       "        [ 1.20239928e+00, -9.74011492e-02,  3.77487621e-03,\n",
       "         -1.72963510e-01,  9.88784953e-02, -3.56022129e-01,\n",
       "         -1.81739260e-01, -2.02771926e+00,  2.21417161e-01,\n",
       "         -4.43248765e+00,  2.06053972e+00],\n",
       "        [ 1.21183740e+00, -4.37770065e-03,  2.12607694e-03,\n",
       "         -1.22913729e-03, -1.89244258e-02, -3.48481807e-01,\n",
       "         -7.79706619e-02, -7.77012921e-01,  6.45593334e-04,\n",
       "         -1.15353028e+00,  1.42911231e+00]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=array([1., 1., 1., 0., 1., 1., 0., 0., 0., 0.])>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer1.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956d47d",
   "metadata": {},
   "source": [
    "### Soft Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7377d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    \n",
    "    def __init__(self, env, observation_dimensions, action_dimensions, action_bound, buffer_capacity,\n",
    "                 minibatch_size=256, gamma=0.99, tau=0.95, lr=3e-4):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.a = Actor(action_dimensions, action_bound)\n",
    "        self.c_gen = Critic_Wrapper(observation_dimensions, action_dimensions)\n",
    "        self.c1 = self.c_gen.get_critic()\n",
    "        self.c2 = self.c_gen.get_critic()\n",
    "        self.tc1 = self.c_gen.get_critic()\n",
    "        self.tc2 = self.c_gen.get_critic()\n",
    "        \n",
    "        self.tc1.set_weights(self.c1.get_weights())\n",
    "        self.tc2.set_weights(self.c2.get_weights())\n",
    "\n",
    "        self.te = -np.prod(action_dimensions)\n",
    "        self.alpha = tf.Variable(0.0, dtype=tf.float32)\n",
    "        \n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c1_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c2_opt = tf.keras.optimizers.Adam(learning_rate=lr)                                                  \n",
    "        self.alpha_opt = tf.keras.optimizers.Adam(learning_rate=lr)   \n",
    "        \n",
    "        self.buffer = Buffer(observation_dimensions, action_dimensions, buffer_capacity, minibatch_size)\n",
    "        \n",
    "        self.gamma, self.tau = gamma, tau\n",
    "        \n",
    "    def train(self, max_env_step):\n",
    "        t = 0\n",
    "        epo = 0\n",
    "        while t < max_env_step:\n",
    "            p_s = self.env.reset()\n",
    "            a_losses = []\n",
    "            c1_losses = []\n",
    "            c2_losses = []\n",
    "            alpha_losses = []\n",
    "            while True:\n",
    "                a, log_a = self.a(tf.expand_dims(p_s, 0))\n",
    "                a=a[0]\n",
    "                s, r, d, _ = self.env.step(a)\n",
    "                end = 0 if d else 1\n",
    "                \n",
    "                self.buffer.record((p_s, a, r, s, end))\n",
    "                data = self.buffer.sample()\n",
    "                \n",
    "                a_loss, c1_loss, c2_loss, alpha_loss = self.update(data)\n",
    "                \n",
    "                a_losses.append(a_loss.numpy())\n",
    "                c1_losses.append(c1_loss.numpy())\n",
    "                c2_losses.append(c2_loss.numpy())\n",
    "                alpha_losses.append(alpha_loss.numpy())\n",
    "                \n",
    "                t = t+1\n",
    "                \n",
    "                if d:\n",
    "                    break\n",
    "                p_s = s\n",
    "                \n",
    "            print(\"Epoch {:04d}\".format(epo), \"Policy Avg. Loss: \", np.mean(a_losses), \n",
    "                  \", Critic 1 Avg. Loss: \",  np.mean(c1_losses), \n",
    "                  \", Critic 2 Avg. Loss: \",  np.mean(c2_losses), \n",
    "                  \", Alpha 1 Avg. Loss: \",  np.mean(alpha_losses), flush=True)\n",
    "            epo = epo+1\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def update(self, data):\n",
    "        s_b, a_b, r_b, ns_b, d_b = data\n",
    "        with tf.GradientTape() as tape_c1, tf.GradientTape() as tape_c2:\n",
    "            q1 = self.c1([s_b, a_b])\n",
    "            q2 = self.c2([s_b, a_b])\n",
    "            na, nlog_a = self.a(ns_b)\n",
    "            \n",
    "            tq1 = self.tc1([ns_b, na])\n",
    "            tq2 = self.tc2([ns_b, na])\n",
    "            \n",
    "            min_qt = tf.math.minimum(tq1,tq2)\n",
    "            \n",
    "            soft_qt = min_qt - (self.alpha*nlog_a)\n",
    "            \n",
    "            y = tf.stop_gradient(r_b+self.gamma*d_b*tf.cast(soft_qt, dtype=tf.float64))\n",
    "            \n",
    "            L_c1 = 0.5*tf.reduce_mean((y-tf.cast(q1, dtype=tf.float64))**2)\n",
    "            L_c2 = 0.5*tf.reduce_mean((y-tf.cast(q2, dtype=tf.float64))**2)\n",
    "        c1_grad = tape_c1.gradient(L_c1, self.c1.trainable_variables)\n",
    "        c2_grad = tape_c2.gradient(L_c2, self.c2.trainable_variables)\n",
    "        \n",
    "        self.c1_opt.apply_gradients(zip(c1_grad, self.c1.trainable_variables))\n",
    "        self.c2_opt.apply_gradients(zip(c2_grad, self.c2.trainable_variables))\n",
    "        \n",
    "        for (tc1w, c1w) in zip(self.tc1.variables, self.c1.variables):\n",
    "            tc1w.assign(tc1w*self.tau + c1w*(1.0-self.tau))\n",
    "        for (tc2w, c2w) in zip(self.tc2.variables, self.c2.variables):\n",
    "            tc2w.assign(tc2w*self.tau + c2w*(1.0-self.tau))\n",
    "            \n",
    "        with tf.GradientTape() as tape_a, tf.GradientTape() as tape_alpha:\n",
    "            a, log_a = self.a(s_b)\n",
    "            qa1 = self.c1([s_b, a])\n",
    "            qa2 = self.c2([s_b, a])\n",
    "            \n",
    "            soft_qa = tf.reduce_mean([qa1,qa2], axis=0)\n",
    "            \n",
    "            L_a = -tf.reduce_mean(soft_qa-self.alpha*log_a)\n",
    "            L_alpha = -tf.reduce_mean(self.alpha*tf.stop_gradient(log_a + self.te))\n",
    "        grad_a = tape_a.gradient(L_a, self.a.trainable_variables)\n",
    "        grad_alpha = tape_alpha.gradient(L_alpha, [self.alpha])\n",
    "        self.a_opt.apply_gradients(zip(grad_a, self.a.trainable_variables))\n",
    "        self.alpha_opt.apply_gradients(zip(grad_alpha, [self.alpha]))\n",
    "        \n",
    "        return L_a, L_c1, L_c2, L_alpha\n",
    "    \n",
    "    def save_weights(self, dir_path):\n",
    "        self.a.save_weights(dir_path+\"/a.ckpt\")\n",
    "        print(\"Saved actor weights\", flush=True)\n",
    "        self.c1.save_weights(dir_path+\"/c1.ckpt\")\n",
    "        print(\"Saved critic 1 weights\", flush=True)\n",
    "        self.c2.save_weights(dir_path+\"/c2.ckpt\")\n",
    "        print(\"Saved critic 2 weights\", flush=True)\n",
    "\n",
    "    def load_weights(self, dir_path):\n",
    "        try:\n",
    "            self.a.load_weights(dir_path+\"/a.ckpt\")\n",
    "            print(\"Loaded actor weights\", flush=True)\n",
    "            self.c1.load_weights(dir_path+\"/c1.ckpt\")\n",
    "            print(\"Loaded critic 1 weights\", flush=True)\n",
    "            self.c2.load_weights(dir_path+\"/c2.ckpt\")\n",
    "            print(\"Loaded critic 2 weights\", flush=True)\n",
    "        except ValueError:\n",
    "            print(\"ERROR: Please make sure weights are saved as .ckpt\", flush=True)\n",
    "            \n",
    "    def eval_rollout(self, problem):\n",
    "        eps_r = 0\n",
    "        eval_env = gym.make(problem)\n",
    "        eval_obs = eval_env.reset()\n",
    "\n",
    "        while True:\n",
    "            eval_env.render()\n",
    "\n",
    "            tf_eval_obs = tf.expand_dims(tf.convert_to_tensor(eval_obs), 0)\n",
    "\n",
    "            eval_a, eval_log_a = self.a(tf_eval_obs, eval_mode=True)\n",
    "\n",
    "            eval_a = eval_a[0]\n",
    "\n",
    "            eval_obs_new, eval_r, eval_d, _ = eval_env.step(eval_a)\n",
    "\n",
    "            eps_r += eval_r\n",
    "\n",
    "            if eval_d:\n",
    "                break\n",
    "                \n",
    "            eval_obs = eval_obs_new\n",
    "\n",
    "        glfw.destroy_window(eval_env.viewer.window)\n",
    "        eval_env.close()\n",
    "        print(\"rollout episodic reward: \", eps_r, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75bddba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac1 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ba42cdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/robocanes/miniconda3/envs/rl_models/lib/python3.8/site-packages/tensorflow_probability/python/distributions/distribution.py:342: calling MultivariateNormalDiag.__init__ (from tensorflow_probability.python.distributions.mvn_diag) with scale_identity_multiplier is deprecated and will be removed after 2020-01-01.\n",
      "Instructions for updating:\n",
      "`scale_identity_multiplier` is deprecated; please combine it into `scale_diag` directly instead.\n",
      "Epoch 0000 Policy Avg. Loss:  -0.17323576 , Critic 1 Avg. Loss:  0.23493227020204865 , Critic 2 Avg. Loss:  0.22517719571923336 , Alpha 1 Avg. Loss:  -0.016376026\n",
      "Epoch 0001 Policy Avg. Loss:  -0.4421635 , Critic 1 Avg. Loss:  0.1458823643086784 , Critic 2 Avg. Loss:  0.17533609958233584 , Alpha 1 Avg. Loss:  -0.02802293\n",
      "Epoch 0002 Policy Avg. Loss:  -0.69007957 , Critic 1 Avg. Loss:  0.1272448478755582 , Critic 2 Avg. Loss:  0.1835665959286314 , Alpha 1 Avg. Loss:  -0.031876553\n",
      "Epoch 0003 Policy Avg. Loss:  -1.0074254 , Critic 1 Avg. Loss:  0.1579946972764552 , Critic 2 Avg. Loss:  0.2038181972036123 , Alpha 1 Avg. Loss:  -0.031908013\n",
      "Epoch 0004 Policy Avg. Loss:  -1.3389686 , Critic 1 Avg. Loss:  0.19787382896290398 , Critic 2 Avg. Loss:  0.23847185554383746 , Alpha 1 Avg. Loss:  -0.0073544085\n",
      "Epoch 0005 Policy Avg. Loss:  -1.788686 , Critic 1 Avg. Loss:  0.31167988053297746 , Critic 2 Avg. Loss:  0.32136675043779794 , Alpha 1 Avg. Loss:  0.025926294\n",
      "Epoch 0006 Policy Avg. Loss:  -2.5083027 , Critic 1 Avg. Loss:  0.41329950937331705 , Critic 2 Avg. Loss:  0.4096101510575501 , Alpha 1 Avg. Loss:  0.09496304\n",
      "Epoch 0007 Policy Avg. Loss:  -2.7388864 , Critic 1 Avg. Loss:  0.5112143250210979 , Critic 2 Avg. Loss:  0.49916691905272137 , Alpha 1 Avg. Loss:  0.06246777\n",
      "Epoch 0008 Policy Avg. Loss:  -3.1719189 , Critic 1 Avg. Loss:  0.5323542279557892 , Critic 2 Avg. Loss:  0.5133189284060711 , Alpha 1 Avg. Loss:  0.08834988\n",
      "Epoch 0009 Policy Avg. Loss:  -3.4018388 , Critic 1 Avg. Loss:  0.5952760643316511 , Critic 2 Avg. Loss:  0.5470165600951408 , Alpha 1 Avg. Loss:  0.02977039\n",
      "Epoch 0010 Policy Avg. Loss:  -3.512618 , Critic 1 Avg. Loss:  0.5173308576087261 , Critic 2 Avg. Loss:  0.4689246068336975 , Alpha 1 Avg. Loss:  -0.04013816\n",
      "Epoch 0011 Policy Avg. Loss:  -3.5450172 , Critic 1 Avg. Loss:  0.434027381559739 , Critic 2 Avg. Loss:  0.4089300471357708 , Alpha 1 Avg. Loss:  -0.11737375\n",
      "Epoch 0012 Policy Avg. Loss:  -3.4471705 , Critic 1 Avg. Loss:  0.4230232284529003 , Critic 2 Avg. Loss:  0.4030576064679051 , Alpha 1 Avg. Loss:  -0.24876256\n",
      "Epoch 0013 Policy Avg. Loss:  -3.3350155 , Critic 1 Avg. Loss:  0.3341988542130746 , Critic 2 Avg. Loss:  0.30810730894892113 , Alpha 1 Avg. Loss:  -0.2899382\n",
      "Epoch 0014 Policy Avg. Loss:  -3.098645 , Critic 1 Avg. Loss:  0.3228766471500249 , Critic 2 Avg. Loss:  0.30231086953009834 , Alpha 1 Avg. Loss:  -0.43033886\n",
      "Epoch 0015 Policy Avg. Loss:  -2.756431 , Critic 1 Avg. Loss:  0.2897462430039251 , Critic 2 Avg. Loss:  0.2777055724559338 , Alpha 1 Avg. Loss:  -0.5540568\n",
      "Epoch 0016 Policy Avg. Loss:  -2.5731583 , Critic 1 Avg. Loss:  0.309453833340904 , Critic 2 Avg. Loss:  0.3020931619801269 , Alpha 1 Avg. Loss:  -0.6088964\n",
      "Epoch 0017 Policy Avg. Loss:  -2.5541582 , Critic 1 Avg. Loss:  0.29900377980988113 , Critic 2 Avg. Loss:  0.3189960370288213 , Alpha 1 Avg. Loss:  -0.48322207\n",
      "Epoch 0018 Policy Avg. Loss:  -2.368433 , Critic 1 Avg. Loss:  0.3650848357650453 , Critic 2 Avg. Loss:  0.36862286527956645 , Alpha 1 Avg. Loss:  -0.44642866\n",
      "Epoch 0019 Policy Avg. Loss:  -2.0453386 , Critic 1 Avg. Loss:  0.44923597869245757 , Critic 2 Avg. Loss:  0.4301534916778325 , Alpha 1 Avg. Loss:  -0.37868434\n",
      "Epoch 0020 Policy Avg. Loss:  -2.1429527 , Critic 1 Avg. Loss:  0.44611493848816697 , Critic 2 Avg. Loss:  0.44379218934311127 , Alpha 1 Avg. Loss:  -0.32565048\n",
      "Epoch 0021 Policy Avg. Loss:  -2.4143493 , Critic 1 Avg. Loss:  0.455166299170061 , Critic 2 Avg. Loss:  0.44203503122252086 , Alpha 1 Avg. Loss:  -0.2736287\n",
      "Epoch 0022 Policy Avg. Loss:  -3.0474446 , Critic 1 Avg. Loss:  0.4134618757423381 , Critic 2 Avg. Loss:  0.3944477567129771 , Alpha 1 Avg. Loss:  -0.1139448\n",
      "Epoch 0023 Policy Avg. Loss:  -4.024408 , Critic 1 Avg. Loss:  0.3761813383016223 , Critic 2 Avg. Loss:  0.3516732118998177 , Alpha 1 Avg. Loss:  -0.22364095\n",
      "Epoch 0024 Policy Avg. Loss:  -7.2991195 , Critic 1 Avg. Loss:  0.7494953476574863 , Critic 2 Avg. Loss:  0.6703420733043663 , Alpha 1 Avg. Loss:  -0.28823432\n",
      "Epoch 0025 Policy Avg. Loss:  -11.241566 , Critic 1 Avg. Loss:  0.9759076833407346 , Critic 2 Avg. Loss:  0.8438729253600233 , Alpha 1 Avg. Loss:  -0.29430908\n",
      "Epoch 0026 Policy Avg. Loss:  -13.151455 , Critic 1 Avg. Loss:  1.104164130879427 , Critic 2 Avg. Loss:  1.033672273264603 , Alpha 1 Avg. Loss:  -0.25349918\n",
      "Epoch 0027 Policy Avg. Loss:  -14.375664 , Critic 1 Avg. Loss:  1.5604915489597573 , Critic 2 Avg. Loss:  1.5116626780110158 , Alpha 1 Avg. Loss:  -0.31569532\n",
      "Epoch 0028 Policy Avg. Loss:  -15.3518505 , Critic 1 Avg. Loss:  1.9762615265129522 , Critic 2 Avg. Loss:  1.9295275880738287 , Alpha 1 Avg. Loss:  -0.2937482\n",
      "Epoch 0029 Policy Avg. Loss:  -16.277 , Critic 1 Avg. Loss:  2.3193560518250136 , Critic 2 Avg. Loss:  2.2830966779394823 , Alpha 1 Avg. Loss:  -0.3465332\n",
      "Epoch 0030 Policy Avg. Loss:  -16.621462 , Critic 1 Avg. Loss:  3.16093611059277 , Critic 2 Avg. Loss:  3.0777870445647544 , Alpha 1 Avg. Loss:  -0.1242706\n",
      "Epoch 0031 Policy Avg. Loss:  -18.916409 , Critic 1 Avg. Loss:  3.951380473905684 , Critic 2 Avg. Loss:  3.830981753157868 , Alpha 1 Avg. Loss:  -0.15456767\n",
      "Epoch 0032 Policy Avg. Loss:  -21.65412 , Critic 1 Avg. Loss:  6.283125684582071 , Critic 2 Avg. Loss:  6.242835804792666 , Alpha 1 Avg. Loss:  -0.3306375\n",
      "Epoch 0033 Policy Avg. Loss:  -22.646906 , Critic 1 Avg. Loss:  8.31578275237195 , Critic 2 Avg. Loss:  8.610293726782599 , Alpha 1 Avg. Loss:  -0.895519\n",
      "Epoch 0034 Policy Avg. Loss:  -22.877636 , Critic 1 Avg. Loss:  9.452018613961002 , Critic 2 Avg. Loss:  9.612214323177247 , Alpha 1 Avg. Loss:  -0.8506028\n",
      "Epoch 0035 Policy Avg. Loss:  -23.23119 , Critic 1 Avg. Loss:  9.355632338759403 , Critic 2 Avg. Loss:  9.602181627188598 , Alpha 1 Avg. Loss:  -0.6503655\n",
      "Epoch 0036 Policy Avg. Loss:  -23.103529 , Critic 1 Avg. Loss:  9.214148892701239 , Critic 2 Avg. Loss:  9.574685322840388 , Alpha 1 Avg. Loss:  -1.3695359\n",
      "Epoch 0037 Policy Avg. Loss:  -22.073227 , Critic 1 Avg. Loss:  12.130838565086016 , Critic 2 Avg. Loss:  12.934456261590555 , Alpha 1 Avg. Loss:  -1.54143\n",
      "Epoch 0038 Policy Avg. Loss:  -21.76646 , Critic 1 Avg. Loss:  7.424928474300932 , Critic 2 Avg. Loss:  7.655063334838046 , Alpha 1 Avg. Loss:  -1.0310396\n",
      "Epoch 0039 Policy Avg. Loss:  -21.692753 , Critic 1 Avg. Loss:  9.123124736037033 , Critic 2 Avg. Loss:  9.885246862022369 , Alpha 1 Avg. Loss:  -0.8674262\n",
      "Epoch 0040 Policy Avg. Loss:  -21.070578 , Critic 1 Avg. Loss:  6.8518153340407535 , Critic 2 Avg. Loss:  6.996951213864646 , Alpha 1 Avg. Loss:  -0.6759461\n",
      "Epoch 0041 Policy Avg. Loss:  -21.201736 , Critic 1 Avg. Loss:  7.651113636619022 , Critic 2 Avg. Loss:  8.270060586677614 , Alpha 1 Avg. Loss:  -0.34897956\n",
      "Epoch 0042 Policy Avg. Loss:  -19.90955 , Critic 1 Avg. Loss:  7.312027812592501 , Critic 2 Avg. Loss:  7.697695518949791 , Alpha 1 Avg. Loss:  -1.064252\n",
      "Epoch 0043 Policy Avg. Loss:  -19.824312 , Critic 1 Avg. Loss:  5.420604753625808 , Critic 2 Avg. Loss:  5.610094152535989 , Alpha 1 Avg. Loss:  -0.22432333\n",
      "Epoch 0044 Policy Avg. Loss:  -19.235205 , Critic 1 Avg. Loss:  4.397560994235195 , Critic 2 Avg. Loss:  4.5611381302276595 , Alpha 1 Avg. Loss:  -0.88691276\n",
      "Epoch 0045 Policy Avg. Loss:  -18.67555 , Critic 1 Avg. Loss:  4.35798151229671 , Critic 2 Avg. Loss:  4.531207778280177 , Alpha 1 Avg. Loss:  -0.28767523\n",
      "Epoch 0046 Policy Avg. Loss:  -18.102968 , Critic 1 Avg. Loss:  3.6621759346027494 , Critic 2 Avg. Loss:  3.778111800932014 , Alpha 1 Avg. Loss:  -0.211172\n",
      "Epoch 0047 Policy Avg. Loss:  -17.62642 , Critic 1 Avg. Loss:  3.726571872249433 , Critic 2 Avg. Loss:  3.780328267450845 , Alpha 1 Avg. Loss:  -0.12465108\n",
      "Epoch 0048 Policy Avg. Loss:  -17.14897 , Critic 1 Avg. Loss:  4.041788586647679 , Critic 2 Avg. Loss:  3.9949033570231793 , Alpha 1 Avg. Loss:  0.06462235\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0049 Policy Avg. Loss:  -16.847857 , Critic 1 Avg. Loss:  3.3761103894118674 , Critic 2 Avg. Loss:  3.296152030299844 , Alpha 1 Avg. Loss:  0.11224912\n",
      "Epoch 0050 Policy Avg. Loss:  -16.121824 , Critic 1 Avg. Loss:  3.034699540538434 , Critic 2 Avg. Loss:  3.0491944833148543 , Alpha 1 Avg. Loss:  -0.16300718\n",
      "Epoch 0051 Policy Avg. Loss:  -16.246952 , Critic 1 Avg. Loss:  2.5643210310657985 , Critic 2 Avg. Loss:  2.613567626371913 , Alpha 1 Avg. Loss:  -0.2824378\n",
      "Epoch 0052 Policy Avg. Loss:  -16.179592 , Critic 1 Avg. Loss:  2.700810109046626 , Critic 2 Avg. Loss:  2.7536229158395846 , Alpha 1 Avg. Loss:  -0.28990152\n"
     ]
    }
   ],
   "source": [
    "sac1.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94e70621",
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "/Users; Permission denied",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msac1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/Users/anthonylaw/Desktop/Work_Research/rl_models/MA2C/sac/devel/test_out/weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mSAC.save_weights\u001b[0;34m(self, dir_path)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_weights\u001b[39m(\u001b[38;5;28mself\u001b[39m, dir_path):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdir_path\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/a.ckpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaved actor weights\u001b[39m\u001b[38;5;124m\"\u001b[39m, flush\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc1\u001b[38;5;241m.\u001b[39msave_weights(dir_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/c1.ckpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/rl_models/lib/python3.8/site-packages/tensorflow/python/lib/io/file_io.py:511\u001b[0m, in \u001b[0;36mrecursive_create_dir_v2\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio.gfile.makedirs\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    500\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrecursive_create_dir_v2\u001b[39m(path):\n\u001b[1;32m    501\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a directory and all parent/intermediate directories.\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \n\u001b[1;32m    503\u001b[0m \u001b[38;5;124;03m  It succeeds if path already exists and is writable.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;124;03m    errors.OpError: If the operation fails.\u001b[39;00m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 511\u001b[0m   \u001b[43m_pywrap_file_io\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRecursivelyCreateDir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /Users; Permission denied"
     ]
    }
   ],
   "source": [
    "sac1.save_weights(\"/Users/anthonylaw/Desktop/Work_Research/rl_models/MA2C/sac/devel/test_out/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909c1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac2 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba100f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac2.load_weights(\"/Users/anthonylaw/Desktop/Work_Research/rl_models/MA2C/sac/devel/test_out/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c45be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac2.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f816b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
