{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5dc7ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/pandas/compat/numpy/__init__.py:10: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  _nlv = LooseVersion(_np_version)\n",
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/pandas/compat/numpy/function.py:120: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(__version__) >= LooseVersion(\"1.17.0\"):\n",
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/matplotlib/__init__.py:202: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(module.__version__) < minver:\n",
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/matplotlib/backend_bases.py:59: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if LooseVersion(PILLOW_VERSION) >= \"3.4\":\n",
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/setuptools/_distutils/version.py:351: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  other = LooseVersion(other)\n",
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import regularizers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cca3c",
   "metadata": {},
   "source": [
    "### Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e91a9ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anthonylaw/anaconda3/envs/rl_models/lib/python3.7/site-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "problem = \"Hopper-v3\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafe2ab",
   "metadata": {},
   "source": [
    "### Actor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aeb90",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279be0e3",
   "metadata": {},
   "source": [
    "Same as PPO actor. Gaussian policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e15e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10\n",
    "\n",
    "class Actor(Model):\n",
    "\n",
    "    def __init__(self, action_dimensions, action_bound):\n",
    "        super().__init__()\n",
    "        self.action_dim, self.upper_bound = action_dimensions, action_bound\n",
    "        self.sample_dist = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(self.action_dim),\n",
    "                                                                    scale_diag=tf.ones(self.action_dim))\n",
    "        self.dense1_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.mean_layer = layers.Dense(self.action_dim)\n",
    "        self.stdev_layer = layers.Dense(self.action_dim)\n",
    "\n",
    "    def call(self, state, eval_mode=False):\n",
    "\n",
    "        a1 = self.dense1_layer(state)\n",
    "        a2 = self.dense2_layer(a1)\n",
    "        mu = self.mean_layer(a2)\n",
    "\n",
    "        log_sigma = self.stdev_layer(a2)\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        sigma = tf.clip_by_value(sigma, EPSILON, 2.718)\n",
    "\n",
    "        dist = tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "        \n",
    "        if eval_mode:\n",
    "            action_ = mu\n",
    "        else:\n",
    "            action_ = tf.math.add(mu, tf.math.multiply(sigma, tf.expand_dims(self.sample_dist.sample(), 0)))\n",
    " \n",
    "        action = tf.tanh(action_)\n",
    "\n",
    "        log_pi_ = dist.log_prob(action_)     \n",
    "        log_pi = log_pi_ - tf.reduce_sum(tf.math.log(tf.clip_by_value(1 - action**2, EPSILON, 1.0)), axis=1)\n",
    "        \n",
    "        return action*self.upper_bound, log_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d3f74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_test = Actor(num_actions, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d7cab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.9941358  -0.72490025 -0.6927037 ]], shape=(1, 3), dtype=float32) tf.Tensor([-1.3114829], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs\n",
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "tf_obs\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "print(a_test, log_a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "116539f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               multiple                  768       \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  195       \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,318\n",
      "Trainable params: 5,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fae85",
   "metadata": {},
   "source": [
    "### Critic Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a403d18b",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347d502",
   "metadata": {},
   "source": [
    "Different from PPO, critic evaluate state-action pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d5f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Wrapper():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.s_dim=state_dim\n",
    "        self.a_dim=action_dim\n",
    "        \n",
    "    def get_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(self.s_dim))\n",
    "        state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(self.a_dim))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "        out = layers.Dense(64, activation=\"relu\")(concat)\n",
    "        outputs = tf.squeeze(layers.Dense(1)(out))\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4dadd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_gen = Critic_Wrapper(num_states, num_actions)\n",
    "critic_test = critic_gen.get_critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65aced4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25089241e+00, -2.10895261e-03, -1.11322642e-03,  3.80833621e-03,\n",
       "        2.32433107e-03,  3.59612832e-03, -4.67824255e-03, -4.85445677e-04,\n",
       "       -1.80631925e-03,  4.81813770e-03, -2.55632145e-03])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c101830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11), dtype=float64, numpy=\n",
       "array([[ 1.25089241e+00, -2.10895261e-03, -1.11322642e-03,\n",
       "         3.80833621e-03,  2.32433107e-03,  3.59612832e-03,\n",
       "        -4.67824255e-03, -4.85445677e-04, -1.80631925e-03,\n",
       "         4.81813770e-03, -2.55632145e-03]])>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "tf_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa334304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.038194172>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_test = critic_test([tf_obs, a_test])\n",
    "v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a2cfe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11)\n"
     ]
    }
   ],
   "source": [
    "obs_new, _, _, _ = env.step(a_test[0])\n",
    "tf_obs_new = tf.expand_dims(obs_new, 0)\n",
    "statex2 = tf.convert_to_tensor([obs, obs_new])\n",
    "print(statex2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df584535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2,)\n"
     ]
    }
   ],
   "source": [
    "a_2, loga_2 = actor_test(statex2)\n",
    "\n",
    "print(a_2.shape, loga_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f16a7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "v_2 = critic_test([statex2, a_2])\n",
    "print(v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eb6392ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           384         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           128         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64)           0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           4160        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            65          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  None                0           ['dense_7[0][0]']                \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3c971",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66ff1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, obs_dim, a_dim, buffer_capacity=100000, batch_size=256):\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, self.a_dim))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "        self.buffer_counter += 1\n",
    "        \n",
    "    def sample(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.squeeze(tf.convert_to_tensor(self.reward_buffer[batch_indices]))\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.squeeze(tf.convert_to_tensor(self.done_buffer[batch_indices]))\n",
    "        \n",
    "        return (state_batch,\n",
    "               action_batch,\n",
    "               reward_batch,\n",
    "               next_state_batch,\n",
    "               done_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d040f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer1 = Buffer(num_states, num_actions, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "11879892",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_obs = env.reset()\n",
    "\n",
    "for i in range(buffer1.buffer_capacity):\n",
    "    a, _ = actor_test(tf.expand_dims(prev_obs, 0))\n",
    "    obs, r, d, _ = env.step(a[0])\n",
    "    \n",
    "    buffer1.record((prev_obs, a[0], r, obs, d))\n",
    "    \n",
    "    prev_obs = obs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ac7dac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 5.23663984e-01, -1.13121674e+00, -1.98751898e-01,\n",
       "         -2.62706605e+00, -6.02562130e-01,  1.64063057e+00,\n",
       "         -1.03479354e+00, -1.41634745e+00, -1.89422938e+00,\n",
       "          1.98933195e-01, -6.94132881e+00],\n",
       "        [ 1.08457497e+00, -5.65752572e-01, -1.66194852e-02,\n",
       "         -1.41946917e+00,  7.53706495e-01, -1.63635638e-01,\n",
       "         -1.95810178e+00, -6.61124435e-01,  3.90131515e+00,\n",
       "         -1.00000000e+01, -3.92664112e+00],\n",
       "        [ 1.19934065e+00, -1.96396271e-01,  4.72482934e-03,\n",
       "         -4.74358212e-01,  4.98837054e-01, -3.08678026e-02,\n",
       "         -1.65350566e-01, -2.88059803e+00, -1.09175940e+00,\n",
       "         -5.26309694e+00,  7.67004363e+00],\n",
       "        [ 4.44129089e-01, -1.16710359e+00, -4.19732410e-01,\n",
       "         -2.60585857e+00, -8.04937074e-01,  2.90345238e-01,\n",
       "          2.68244490e-02, -2.10895590e+00, -3.10609059e+00,\n",
       "          9.71435185e-01,  8.70259724e-01],\n",
       "        [ 1.25295866e+00,  2.54342752e-03,  2.82120760e-03,\n",
       "          3.52012277e-04,  4.03621415e-03,  2.75527608e-02,\n",
       "         -1.00860004e-01,  4.15400031e-02,  3.49325151e-03,\n",
       "          6.12134505e-04,  1.21024919e+00],\n",
       "        [ 5.23663984e-01, -1.13121674e+00, -1.98751898e-01,\n",
       "         -2.62706605e+00, -6.02562130e-01,  1.64063057e+00,\n",
       "         -1.03479354e+00, -1.41634745e+00, -1.89422938e+00,\n",
       "          1.98933195e-01, -6.94132881e+00],\n",
       "        [ 1.06826576e+00, -5.71319553e-01,  1.48078736e-02,\n",
       "         -1.50140423e+00,  7.17358454e-01, -2.22543363e-01,\n",
       "         -2.16954376e+00, -1.54916577e+00,  2.90447418e+00,\n",
       "         -1.00000000e+01, -5.15440603e+00],\n",
       "        [ 1.20662116e+00, -5.24919490e-02,  8.08552463e-03,\n",
       "         -1.36981781e-01,  4.96392424e-02, -1.11014486e-01,\n",
       "          9.47679244e-02,  8.59585689e-01,  2.19643985e+00,\n",
       "         -1.85684690e+00,  1.13232973e+00],\n",
       "        [ 4.48371633e-01, -1.13872223e+00, -3.68378645e-01,\n",
       "         -2.61778687e+00, -8.22211046e-01,  1.21683399e+00,\n",
       "         -5.52504906e-01, -4.37675031e-01, -2.85549526e+00,\n",
       "          8.95960293e-01,  8.10013166e-01],\n",
       "        [ 1.20390631e+00, -6.31564384e-02, -2.81990711e-02,\n",
       "         -9.99108630e-02,  4.56999222e-02, -1.39913303e-01,\n",
       "          2.08576049e-01,  4.99554340e-02,  1.15893392e+00,\n",
       "         -1.63684367e+00, -4.50789043e-01]])>,\n",
       " <tf.Tensor: shape=(10, 3), dtype=float64, numpy=\n",
       " array([[ 0.12304067, -0.93425149,  0.99742514],\n",
       "        [ 0.9156481 , -0.27663115, -0.85634011],\n",
       "        [ 0.15931115,  0.82705265,  0.22720425],\n",
       "        [-0.96144915, -0.62901795, -0.4874301 ],\n",
       "        [ 0.2687951 , -0.76532608,  0.5866648 ],\n",
       "        [ 0.12304067, -0.93425149,  0.99742514],\n",
       "        [ 0.38129702, -0.26441416, -0.23309366],\n",
       "        [ 0.3139742 , -0.60650891,  0.83500075],\n",
       "        [-0.45039383, -0.61347407,  0.74737573],\n",
       "        [ 0.79456705, -0.89158261,  0.22465657]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       " array([2.67784876, 0.82121707, 1.04766146, 1.15798356, 0.98526495,\n",
       "        2.67784876, 0.73909157, 0.82445355, 1.81115026, 0.82120018])>,\n",
       " <tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 5.15688973e-01, -1.14009821e+00, -2.13698591e-01,\n",
       "         -2.62569134e+00, -6.52698959e-01,  1.71480689e+00,\n",
       "         -9.61382253e-01, -7.91608216e-01, -1.84052719e+00,\n",
       "          1.47693699e-01, -5.60360013e+00],\n",
       "        [ 1.06826576e+00, -5.71319553e-01,  1.48078736e-02,\n",
       "         -1.50140423e+00,  7.17358454e-01, -2.22543363e-01,\n",
       "         -2.16954376e+00, -1.54916577e+00,  2.90447418e+00,\n",
       "         -1.00000000e+01, -5.15440603e+00],\n",
       "        [ 1.19829055e+00, -2.16906480e-01, -3.63551779e-03,\n",
       "         -5.12828250e-01,  5.60168336e-01,  1.28173752e-01,\n",
       "         -9.38462237e-02, -2.24718577e+00, -9.99513570e-01,\n",
       "         -4.35998009e+00,  7.66140714e+00],\n",
       "        [ 4.43513127e-01, -1.19025010e+00, -4.49610186e-01,\n",
       "         -2.59926531e+00, -7.99182894e-01,  4.81217782e-02,\n",
       "         -1.95750895e-01, -3.64749577e+00, -4.35950689e+00,\n",
       "          6.76181101e-01,  5.88009121e-01],\n",
       "        [ 1.25181209e+00,  9.00665098e-04,  2.73966646e-03,\n",
       "         -3.91975217e-03,  1.73007634e-02, -5.47561917e-02,\n",
       "         -1.86116227e-01, -4.48939505e-01, -2.04377061e-02,\n",
       "         -1.06759671e+00,  2.10457087e+00],\n",
       "        [ 5.15688973e-01, -1.14009821e+00, -2.13698591e-01,\n",
       "         -2.62569134e+00, -6.52698959e-01,  1.71480689e+00,\n",
       "         -9.61382253e-01, -7.91608216e-01, -1.84052719e+00,\n",
       "          1.47693699e-01, -5.60360013e+00],\n",
       "        [ 1.04978996e+00, -5.90563142e-01,  2.95632934e-02,\n",
       "         -1.58290300e+00,  6.74873616e-01, -2.92016149e-01,\n",
       "         -2.44039120e+00, -3.09975124e+00,  9.89677762e-01,\n",
       "         -1.00000000e+01, -5.46749455e+00],\n",
       "        [ 1.20689236e+00, -5.20942403e-02,  1.94445445e-02,\n",
       "         -1.54782433e-01,  6.37192823e-02, -2.30647840e-01,\n",
       "         -2.71030520e-02, -6.38980589e-01,  7.92103757e-01,\n",
       "         -2.60392015e+00,  2.38439528e+00],\n",
       "        [ 4.45141406e-01, -1.14971601e+00, -3.93556823e-01,\n",
       "         -2.61195992e+00, -8.13678955e-01,  4.83621956e-01,\n",
       "         -3.06205075e-01, -2.16948745e+00, -3.43739508e+00,\n",
       "          5.33375857e-01,  1.35087807e+00],\n",
       "        [ 1.20548854e+00, -6.08265151e-02, -1.38114501e-02,\n",
       "         -1.17870256e-01,  4.45203633e-02, -2.14146267e-01,\n",
       "          1.83372383e-01,  5.30438891e-01,  2.43615788e+00,\n",
       "         -2.85325373e+00,  1.48980908e-01]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=array([1., 1., 1., 1., 0., 1., 1., 0., 1., 0.])>)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer1.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956d47d",
   "metadata": {},
   "source": [
    "### Soft Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7377d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    \n",
    "    def __init__(self, env, observation_dimensions, action_dimensions, action_bound, buffer_capacity,\n",
    "                 minibatch_size=256, gamma=0.99, tau=0.95, lr=3e-4):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.a = Actor(action_dimensions, action_bound)\n",
    "        self.c_gen = Critic_Wrapper(observation_dimensions, action_dimensions)\n",
    "        self.c1 = self.c_gen.get_critic()\n",
    "        self.c2 = self.c_gen.get_critic()\n",
    "        self.tc1 = self.c_gen.get_critic()\n",
    "        self.tc2 = self.c_gen.get_critic()\n",
    "        \n",
    "        self.tc1.set_weights(self.c1.get_weights())\n",
    "        self.tc2.set_weights(self.c2.get_weights())\n",
    "\n",
    "        self.te = -np.prod(action_dimensions)\n",
    "        self.alpha = tf.Variable(0.0, dtype=tf.float32)\n",
    "        \n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c1_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c2_opt = tf.keras.optimizers.Adam(learning_rate=lr)                                                  \n",
    "        self.alpha_opt = tf.keras.optimizers.Adam(learning_rate=lr)   \n",
    "        \n",
    "        self.buffer = Buffer(observation_dimensions, action_dimensions, buffer_capacity, minibatch_size)\n",
    "        \n",
    "        self.gamma, self.tau = gamma, tau\n",
    "        \n",
    "    def train(self, max_env_step):\n",
    "        t = 0\n",
    "        epo = 0\n",
    "        while t < max_env_step:\n",
    "            p_s = self.env.reset()\n",
    "            a_losses = []\n",
    "            c1_losses = []\n",
    "            c2_losses = []\n",
    "            alpha_losses = []\n",
    "            while True:\n",
    "                a, log_a = self.a(tf.expand_dims(p_s, 0))\n",
    "                a=a[0]\n",
    "                s, r, d, _ = self.env.step(a)\n",
    "                end = 0 if d else 1\n",
    "                \n",
    "                self.buffer.record((p_s, a, r, s, end))\n",
    "                data = self.buffer.sample()\n",
    "                \n",
    "                a_loss, c1_loss, c2_loss, alpha_loss = self.update(data)\n",
    "                \n",
    "                a_losses.append(a_loss.numpy())\n",
    "                c1_losses.append(c1_loss.numpy())\n",
    "                c2_losses.append(c2_loss.numpy())\n",
    "                alpha_losses.append(alpha_loss.numpy())\n",
    "                \n",
    "                t = t+1\n",
    "                \n",
    "                if d:\n",
    "                    break\n",
    "                p_s = s\n",
    "                \n",
    "            print(\"Epoch {:04d}\".format(epo), \"Policy Avg. Loss: \", np.mean(a_losses), \n",
    "                  \", Critic 1 Avg. Loss: \",  np.mean(c1_losses), \n",
    "                  \", Critic 2 Avg. Loss: \",  np.mean(c2_losses), \n",
    "                  \", Alpha 1 Avg. Loss: \",  np.mean(alpha_losses), flush=True)\n",
    "            epo = epo+1\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def update(self, data):\n",
    "        s_b, a_b, r_b, ns_b, d_b = data\n",
    "        with tf.GradientTape() as tape_c1, tf.GradientTape() as tape_c2:\n",
    "            q1 = self.c1([s_b, a_b])\n",
    "            q2 = self.c2([s_b, a_b])\n",
    "            na, nlog_a = self.a(ns_b)\n",
    "            \n",
    "            tq1 = self.tc1([ns_b, na])\n",
    "            tq2 = self.tc2([ns_b, na])\n",
    "            \n",
    "            min_qt = tf.math.minimum(tq1,tq2)\n",
    "            \n",
    "            soft_qt = min_qt - (self.alpha*nlog_a)\n",
    "            \n",
    "            y = tf.stop_gradient(r_b+self.gamma*d_b*tf.cast(soft_qt, dtype=tf.float64))\n",
    "            \n",
    "            L_c1 = 0.5*tf.reduce_mean((y-tf.cast(q1, dtype=tf.float64))**2)\n",
    "            L_c2 = 0.5*tf.reduce_mean((y-tf.cast(q2, dtype=tf.float64))**2)\n",
    "        c1_grad = tape_c1.gradient(L_c1, self.c1.trainable_variables)\n",
    "        c2_grad = tape_c2.gradient(L_c2, self.c2.trainable_variables)\n",
    "        \n",
    "        self.c1_opt.apply_gradients(zip(c1_grad, self.c1.trainable_variables))\n",
    "        self.c2_opt.apply_gradients(zip(c2_grad, self.c2.trainable_variables))\n",
    "        \n",
    "        for (tc1w, c1w) in zip(self.tc1.variables, self.c1.variables):\n",
    "            tc1w.assign(tc1w*self.tau + c1w*(1.0-self.tau))\n",
    "        for (tc2w, c2w) in zip(self.tc2.variables, self.c2.variables):\n",
    "            tc2w.assign(tc2w*self.tau + c2w*(1.0-self.tau))\n",
    "            \n",
    "        with tf.GradientTape() as tape_a, tf.GradientTape() as tape_alpha:\n",
    "            a, log_a = self.a(s_b)\n",
    "            qa1 = self.c1([s_b, a])\n",
    "            qa2 = self.c2([s_b, a])\n",
    "            \n",
    "            soft_qa = tf.reduce_mean([qa1,qa2], axis=0)\n",
    "            \n",
    "            L_a = -tf.reduce_mean(soft_qa-self.alpha*log_a)\n",
    "            L_alpha = -tf.reduce_mean(self.alpha*tf.stop_gradient(log_a + self.te))\n",
    "        grad_a = tape_a.gradient(L_a, self.a.trainable_variables)\n",
    "        grad_alpha = tape_alpha.gradient(L_alpha, [self.alpha])\n",
    "        self.a_opt.apply_gradients(zip(grad_a, self.a.trainable_variables))\n",
    "        self.alpha_opt.apply_gradients(zip(grad_alpha, [self.alpha]))\n",
    "        \n",
    "        return L_a, L_c1, L_c2, L_alpha\n",
    "    \n",
    "    def save_weights(self, dir_path):\n",
    "        self.a.save_weights(dir_path+\"/a.ckpt\")\n",
    "        print(\"Saved actor weights\", flush=True)\n",
    "        self.c1.save_weights(dir_path+\"/c1.ckpt\")\n",
    "        print(\"Saved critic 1 weights\", flush=True)\n",
    "        self.c2.save_weights(dir_path+\"/c2.ckpt\")\n",
    "        print(\"Saved critic 2 weights\", flush=True)\n",
    "\n",
    "    def load_weights(self, dir_path):\n",
    "        try:\n",
    "            self.a.load_weights(dir_path+\"/a.ckpt\")\n",
    "            print(\"Loaded actor weights\", flush=True)\n",
    "            self.c1.load_weights(dir_path+\"/c1.ckpt\")\n",
    "            print(\"Loaded critic 1 weights\", flush=True)\n",
    "            self.c2.load_weights(dir_path+\"/c2.ckpt\")\n",
    "            print(\"Loaded critic 2 weights\", flush=True)\n",
    "        except ValueError:\n",
    "            print(\"ERROR: Please make sure weights are saved as .ckpt\", flush=True)\n",
    "            \n",
    "    def eval_rollout(self, eval_env):\n",
    "        eps_r = 0\n",
    "        eval_obs = eval_env.reset()\n",
    "        \n",
    "        while True:\n",
    "            eval_env.render()\n",
    "\n",
    "            tf_eval_obs = tf.expand_dims(tf.convert_to_tensor(eval_obs), 0)\n",
    "\n",
    "            eval_a, eval_log_a = self.a(tf_eval_obs, eval_mode=True)\n",
    "\n",
    "            eval_a = eval_a[0]\n",
    "\n",
    "            eval_obs_new, eval_r, eval_d, _ = eval_env.step(eval_a)\n",
    "\n",
    "            eps_r += eval_r\n",
    "\n",
    "            if eval_d:\n",
    "                break\n",
    "                \n",
    "            eval_obs = eval_obs_new\n",
    "        eval_env.close()\n",
    "        print(\"rollout episodic reward: \", eps_r, flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75bddba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac1 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0ba42cdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Policy Avg. Loss:  -0.07243134 , Critic 1 Avg. Loss:  0.22062063767155676 , Critic 2 Avg. Loss:  0.3344533875284732 , Alpha 1 Avg. Loss:  -0.0038230228\n",
      "Epoch 0001 Policy Avg. Loss:  -0.2707948 , Critic 1 Avg. Loss:  0.15812808112825003 , Critic 2 Avg. Loss:  0.17292359268355023 , Alpha 1 Avg. Loss:  -0.011478158\n",
      "Epoch 0002 Policy Avg. Loss:  -0.3884465 , Critic 1 Avg. Loss:  0.1412082966394509 , Critic 2 Avg. Loss:  0.12594620861978278 , Alpha 1 Avg. Loss:  -0.018301968\n",
      "Epoch 0003 Policy Avg. Loss:  -0.40722534 , Critic 1 Avg. Loss:  0.13687380721694256 , Critic 2 Avg. Loss:  0.14382856831562876 , Alpha 1 Avg. Loss:  -0.036046352\n",
      "Epoch 0004 Policy Avg. Loss:  -0.5636535 , Critic 1 Avg. Loss:  0.13242284611894808 , Critic 2 Avg. Loss:  0.20022134601997435 , Alpha 1 Avg. Loss:  -0.031472337\n",
      "Epoch 0005 Policy Avg. Loss:  -0.6606414 , Critic 1 Avg. Loss:  0.13944941201231384 , Critic 2 Avg. Loss:  0.15885819223142922 , Alpha 1 Avg. Loss:  -0.033156212\n",
      "Epoch 0006 Policy Avg. Loss:  -0.7735902 , Critic 1 Avg. Loss:  0.14767670308786499 , Critic 2 Avg. Loss:  0.15760174908392324 , Alpha 1 Avg. Loss:  0.007582917\n",
      "Epoch 0007 Policy Avg. Loss:  -0.9542673 , Critic 1 Avg. Loss:  0.16228916749822797 , Critic 2 Avg. Loss:  0.16512921488717314 , Alpha 1 Avg. Loss:  0.016642125\n",
      "Epoch 0008 Policy Avg. Loss:  -1.0718213 , Critic 1 Avg. Loss:  0.17245823177082512 , Critic 2 Avg. Loss:  0.16444829808517106 , Alpha 1 Avg. Loss:  -0.008041249\n",
      "Epoch 0009 Policy Avg. Loss:  -1.2962099 , Critic 1 Avg. Loss:  0.21878006299817426 , Critic 2 Avg. Loss:  0.20149512974004682 , Alpha 1 Avg. Loss:  0.07888051\n",
      "Epoch 0010 Policy Avg. Loss:  -1.4918293 , Critic 1 Avg. Loss:  0.19491790776912046 , Critic 2 Avg. Loss:  0.17931587391298281 , Alpha 1 Avg. Loss:  0.094687864\n",
      "Epoch 0011 Policy Avg. Loss:  -1.6573001 , Critic 1 Avg. Loss:  0.2902271153497801 , Critic 2 Avg. Loss:  0.26985647798445145 , Alpha 1 Avg. Loss:  0.0635909\n",
      "Epoch 0012 Policy Avg. Loss:  -1.8578322 , Critic 1 Avg. Loss:  0.2667746570159162 , Critic 2 Avg. Loss:  0.25089055246250164 , Alpha 1 Avg. Loss:  0.04987635\n",
      "Epoch 0013 Policy Avg. Loss:  -2.0854578 , Critic 1 Avg. Loss:  0.24936025872235393 , Critic 2 Avg. Loss:  0.22417671623804997 , Alpha 1 Avg. Loss:  0.016379718\n",
      "Epoch 0014 Policy Avg. Loss:  -1.939968 , Critic 1 Avg. Loss:  0.23268157761634478 , Critic 2 Avg. Loss:  0.21150158167906774 , Alpha 1 Avg. Loss:  -0.06852896\n",
      "Epoch 0015 Policy Avg. Loss:  -1.8110232 , Critic 1 Avg. Loss:  0.19102118190501075 , Critic 2 Avg. Loss:  0.16910080594568866 , Alpha 1 Avg. Loss:  -0.13714562\n",
      "Epoch 0016 Policy Avg. Loss:  -1.5884312 , Critic 1 Avg. Loss:  0.14645782121653073 , Critic 2 Avg. Loss:  0.12467535551452999 , Alpha 1 Avg. Loss:  -0.2039372\n",
      "Epoch 0017 Policy Avg. Loss:  -1.4315139 , Critic 1 Avg. Loss:  0.18079416444356144 , Critic 2 Avg. Loss:  0.15327059678870425 , Alpha 1 Avg. Loss:  -0.3142027\n",
      "Epoch 0018 Policy Avg. Loss:  -1.2744211 , Critic 1 Avg. Loss:  0.19865428607806623 , Critic 2 Avg. Loss:  0.17259602474896601 , Alpha 1 Avg. Loss:  -0.38691384\n",
      "Epoch 0019 Policy Avg. Loss:  -1.469087 , Critic 1 Avg. Loss:  0.2651153522826985 , Critic 2 Avg. Loss:  0.2304592246033096 , Alpha 1 Avg. Loss:  -0.28909218\n",
      "Epoch 0020 Policy Avg. Loss:  -1.8031721 , Critic 1 Avg. Loss:  0.3116611206528939 , Critic 2 Avg. Loss:  0.2752675309424671 , Alpha 1 Avg. Loss:  -0.20340449\n",
      "Epoch 0021 Policy Avg. Loss:  -2.011434 , Critic 1 Avg. Loss:  0.32550901780923164 , Critic 2 Avg. Loss:  0.2847645811701657 , Alpha 1 Avg. Loss:  -0.17753594\n",
      "Epoch 0022 Policy Avg. Loss:  -2.162147 , Critic 1 Avg. Loss:  0.330894359683087 , Critic 2 Avg. Loss:  0.29243439300580976 , Alpha 1 Avg. Loss:  -0.16466172\n",
      "Epoch 0023 Policy Avg. Loss:  -2.845592 , Critic 1 Avg. Loss:  0.47276125754464626 , Critic 2 Avg. Loss:  0.4214749021518228 , Alpha 1 Avg. Loss:  -0.10776713\n",
      "Epoch 0024 Policy Avg. Loss:  -2.7194638 , Critic 1 Avg. Loss:  0.4284975034838337 , Critic 2 Avg. Loss:  0.3977703356995369 , Alpha 1 Avg. Loss:  -0.25206235\n",
      "Epoch 0025 Policy Avg. Loss:  -3.3417623 , Critic 1 Avg. Loss:  0.5011807040615041 , Critic 2 Avg. Loss:  0.4578904298407641 , Alpha 1 Avg. Loss:  -0.31081632\n",
      "Epoch 0026 Policy Avg. Loss:  -3.6950948 , Critic 1 Avg. Loss:  0.5261964564935719 , Critic 2 Avg. Loss:  0.4784360274451327 , Alpha 1 Avg. Loss:  -0.32208163\n",
      "Epoch 0027 Policy Avg. Loss:  -4.1978555 , Critic 1 Avg. Loss:  0.5931227983695476 , Critic 2 Avg. Loss:  0.5385069852200997 , Alpha 1 Avg. Loss:  -0.22791103\n",
      "Epoch 0028 Policy Avg. Loss:  -5.3497553 , Critic 1 Avg. Loss:  0.633340008460289 , Critic 2 Avg. Loss:  0.6102882411669045 , Alpha 1 Avg. Loss:  -0.20240188\n",
      "Epoch 0029 Policy Avg. Loss:  -6.5739894 , Critic 1 Avg. Loss:  0.7686514100120274 , Critic 2 Avg. Loss:  0.7168146772637388 , Alpha 1 Avg. Loss:  -0.12698995\n",
      "Epoch 0030 Policy Avg. Loss:  -8.363339 , Critic 1 Avg. Loss:  0.8267319549412561 , Critic 2 Avg. Loss:  0.8298845563272311 , Alpha 1 Avg. Loss:  -0.19483285\n",
      "Epoch 0031 Policy Avg. Loss:  -11.416286 , Critic 1 Avg. Loss:  1.1511174878475323 , Critic 2 Avg. Loss:  1.2941930428784263 , Alpha 1 Avg. Loss:  -0.21756935\n",
      "Epoch 0032 Policy Avg. Loss:  -14.133138 , Critic 1 Avg. Loss:  1.5531555396741696 , Critic 2 Avg. Loss:  1.7333890187921965 , Alpha 1 Avg. Loss:  -0.17381729\n",
      "Epoch 0033 Policy Avg. Loss:  -15.929104 , Critic 1 Avg. Loss:  2.041610028907343 , Critic 2 Avg. Loss:  2.2133037231526513 , Alpha 1 Avg. Loss:  -0.21572784\n",
      "Epoch 0034 Policy Avg. Loss:  -17.940779 , Critic 1 Avg. Loss:  2.905798024962008 , Critic 2 Avg. Loss:  3.0895361735663585 , Alpha 1 Avg. Loss:  -0.3286575\n",
      "Epoch 0035 Policy Avg. Loss:  -19.502722 , Critic 1 Avg. Loss:  3.4656390617851756 , Critic 2 Avg. Loss:  3.7041667901127524 , Alpha 1 Avg. Loss:  -0.7381277\n",
      "Epoch 0036 Policy Avg. Loss:  -19.966846 , Critic 1 Avg. Loss:  3.983261479971426 , Critic 2 Avg. Loss:  4.202803387179119 , Alpha 1 Avg. Loss:  -0.62014663\n",
      "Epoch 0037 Policy Avg. Loss:  -20.827581 , Critic 1 Avg. Loss:  4.184501578057178 , Critic 2 Avg. Loss:  4.397829814365842 , Alpha 1 Avg. Loss:  -0.65384126\n",
      "Epoch 0038 Policy Avg. Loss:  -21.25669 , Critic 1 Avg. Loss:  4.191268045651982 , Critic 2 Avg. Loss:  4.38968146051376 , Alpha 1 Avg. Loss:  -0.74410725\n",
      "Epoch 0039 Policy Avg. Loss:  -21.575943 , Critic 1 Avg. Loss:  4.365950572745441 , Critic 2 Avg. Loss:  4.601664634364978 , Alpha 1 Avg. Loss:  -0.55220354\n",
      "Epoch 0040 Policy Avg. Loss:  -21.594921 , Critic 1 Avg. Loss:  5.212739827484505 , Critic 2 Avg. Loss:  5.349895909207735 , Alpha 1 Avg. Loss:  -0.5195505\n",
      "Epoch 0041 Policy Avg. Loss:  -22.065979 , Critic 1 Avg. Loss:  4.947618472843295 , Critic 2 Avg. Loss:  5.080921872426753 , Alpha 1 Avg. Loss:  -0.30046618\n",
      "Epoch 0042 Policy Avg. Loss:  -22.449234 , Critic 1 Avg. Loss:  4.8519903005853715 , Critic 2 Avg. Loss:  4.95264796839729 , Alpha 1 Avg. Loss:  -0.49404714\n",
      "Epoch 0043 Policy Avg. Loss:  -22.589035 , Critic 1 Avg. Loss:  4.468125367689594 , Critic 2 Avg. Loss:  4.5818333845628825 , Alpha 1 Avg. Loss:  -0.4839333\n",
      "Epoch 0044 Policy Avg. Loss:  -22.444715 , Critic 1 Avg. Loss:  4.361149958123401 , Critic 2 Avg. Loss:  4.493116554225645 , Alpha 1 Avg. Loss:  -0.4528064\n",
      "Epoch 0045 Policy Avg. Loss:  -23.062317 , Critic 1 Avg. Loss:  4.191884876091234 , Critic 2 Avg. Loss:  4.3318964511432805 , Alpha 1 Avg. Loss:  -0.2387975\n",
      "Epoch 0046 Policy Avg. Loss:  -22.563648 , Critic 1 Avg. Loss:  4.149038950663917 , Critic 2 Avg. Loss:  4.220537259895109 , Alpha 1 Avg. Loss:  -0.57256764\n",
      "Epoch 0047 Policy Avg. Loss:  -23.164999 , Critic 1 Avg. Loss:  4.71250421921162 , Critic 2 Avg. Loss:  4.732432350977862 , Alpha 1 Avg. Loss:  -0.23740275\n"
     ]
    }
   ],
   "source": [
    "sac1.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94e70621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved actor weights\n",
      "Saved critic 1 weights\n",
      "Saved critic 2 weights\n"
     ]
    }
   ],
   "source": [
    "sac1.save_weights(\"/Users/anthonylaw/Desktop/Work_Research/rl_models/MA2C/sac/devel/test_out/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "909c1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac2 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9ba100f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved actor weights\n",
      "Saved critic 1 weights\n",
      "Saved critic 2 weights\n"
     ]
    }
   ],
   "source": [
    "sac2.load_weights(\"/Users/anthonylaw/Desktop/Work_Research/rl_models/MA2C/sac/devel/test_out/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17c45be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Policy Avg. Loss:  -22.552607 , Critic 1 Avg. Loss:  133.49638950523433 , Critic 2 Avg. Loss:  129.55168005371527 , Alpha 1 Avg. Loss:  -0.0071828915\n",
      "Epoch 0001 Policy Avg. Loss:  -18.767363 , Critic 1 Avg. Loss:  7.316584747076681 , Critic 2 Avg. Loss:  5.795925751358901 , Alpha 1 Avg. Loss:  -0.05522415\n",
      "Epoch 0002 Policy Avg. Loss:  -17.96469 , Critic 1 Avg. Loss:  5.507646484330524 , Critic 2 Avg. Loss:  5.2844075638829064 , Alpha 1 Avg. Loss:  -0.1362014\n",
      "Epoch 0003 Policy Avg. Loss:  -18.942528 , Critic 1 Avg. Loss:  5.564156786934953 , Critic 2 Avg. Loss:  5.366625323636581 , Alpha 1 Avg. Loss:  -0.24017943\n",
      "Epoch 0004 Policy Avg. Loss:  -20.729391 , Critic 1 Avg. Loss:  6.5831684525339575 , Critic 2 Avg. Loss:  6.137281363338495 , Alpha 1 Avg. Loss:  -0.35341176\n",
      "Epoch 0005 Policy Avg. Loss:  -22.160152 , Critic 1 Avg. Loss:  6.340519342927752 , Critic 2 Avg. Loss:  5.843234969346588 , Alpha 1 Avg. Loss:  -0.30649784\n",
      "Epoch 0006 Policy Avg. Loss:  -23.273882 , Critic 1 Avg. Loss:  6.857609555647052 , Critic 2 Avg. Loss:  6.4058535078891135 , Alpha 1 Avg. Loss:  -0.25474438\n",
      "Epoch 0007 Policy Avg. Loss:  -24.581161 , Critic 1 Avg. Loss:  5.993174374679379 , Critic 2 Avg. Loss:  5.708201103577992 , Alpha 1 Avg. Loss:  -0.30201936\n",
      "Epoch 0008 Policy Avg. Loss:  -25.901558 , Critic 1 Avg. Loss:  6.066287683962724 , Critic 2 Avg. Loss:  5.868677962901174 , Alpha 1 Avg. Loss:  -0.22975388\n"
     ]
    }
   ],
   "source": [
    "sac2.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f816b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
