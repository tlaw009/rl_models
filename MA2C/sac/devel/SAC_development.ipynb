{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dc7ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import regularizers\n",
    "import glfw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cca3c",
   "metadata": {},
   "source": [
    "### Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e91a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"Hopper-v3\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafe2ab",
   "metadata": {},
   "source": [
    "### Actor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aeb90",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279be0e3",
   "metadata": {},
   "source": [
    "Same as PPO actor. Gaussian policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e15e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10\n",
    "\n",
    "class Actor(Model):\n",
    "\n",
    "    def __init__(self, action_dimensions, action_bound):\n",
    "        super().__init__()\n",
    "        self.action_dim, self.upper_bound = action_dimensions, action_bound\n",
    "        self.sample_dist = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(self.action_dim),\n",
    "                                                                    scale_diag=tf.ones(self.action_dim))\n",
    "        self.dense1_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.mean_layer = layers.Dense(self.action_dim)\n",
    "        self.stdev_layer = layers.Dense(self.action_dim)\n",
    "\n",
    "    def call(self, state, eval_mode=False):\n",
    "\n",
    "        a1 = self.dense1_layer(state)\n",
    "        a2 = self.dense2_layer(a1)\n",
    "        mu = self.mean_layer(a2)\n",
    "\n",
    "        log_sigma = self.stdev_layer(a2)\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        sigma = tf.clip_by_value(sigma, EPSILON, 2.718)\n",
    "\n",
    "        dist = tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "        \n",
    "        if eval_mode:\n",
    "            action_ = mu\n",
    "        else:\n",
    "            action_ = tf.math.add(mu, tf.math.multiply(sigma, tf.expand_dims(self.sample_dist.sample(), 0)))\n",
    " \n",
    "        action = tf.tanh(action_)\n",
    "\n",
    "        log_pi_ = dist.log_prob(action_)     \n",
    "        log_pi = log_pi_ - tf.reduce_sum(tf.math.log(tf.clip_by_value(1 - action**2, EPSILON, 1.0)), axis=1)\n",
    "        \n",
    "        return action*self.upper_bound, log_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6d3f74b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_test = Actor(num_actions, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2d7cab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.9719501   0.84101444  0.5297327 ]], shape=(1, 3), dtype=float32) tf.Tensor([-0.8370919], shape=(1,), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs\n",
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "tf_obs\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "print(a_test, log_a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "116539f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             multiple                  768       \n",
      "                                                                 \n",
      " dense_9 (Dense)             multiple                  4160      \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  195       \n",
      "                                                                 \n",
      " dense_11 (Dense)            multiple                  195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,318\n",
      "Trainable params: 5,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fae85",
   "metadata": {},
   "source": [
    "### Critic Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a403d18b",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347d502",
   "metadata": {},
   "source": [
    "Different from PPO, critic evaluate state-action pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7d5f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Wrapper():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.s_dim=state_dim\n",
    "        self.a_dim=action_dim\n",
    "        \n",
    "    def get_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(self.s_dim))\n",
    "        state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(self.a_dim))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "        out = layers.Dense(64, activation=\"relu\")(concat)\n",
    "        outputs = tf.squeeze(layers.Dense(1)(out))\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dadd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_gen = Critic_Wrapper(num_states, num_actions)\n",
    "critic_test = critic_gen.get_critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65aced4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25401705e+00, -1.82400569e-03,  1.70315749e-03, -2.07642951e-03,\n",
       "       -2.80744649e-03,  4.37307766e-03, -2.14324323e-03,  9.02038884e-04,\n",
       "        3.32820211e-03, -1.83249526e-03, -1.83357552e-03])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3c101830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11), dtype=float64, numpy=\n",
       "array([[ 1.25401705e+00, -1.82400569e-03,  1.70315749e-03,\n",
       "        -2.07642951e-03, -2.80744649e-03,  4.37307766e-03,\n",
       "        -2.14324323e-03,  9.02038884e-04,  3.32820211e-03,\n",
       "        -1.83249526e-03, -1.83357552e-03]])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "tf_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa334304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=-0.08869143>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_test = critic_test([tf_obs, a_test])\n",
    "v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2a2cfe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11)\n"
     ]
    }
   ],
   "source": [
    "obs_new, _, _, _ = env.step(a_test[0])\n",
    "tf_obs_new = tf.expand_dims(obs_new, 0)\n",
    "statex2 = tf.convert_to_tensor([obs, obs_new])\n",
    "print(statex2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "df584535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2,)\n"
     ]
    }
   ],
   "source": [
    "a_2, loga_2 = actor_test(statex2)\n",
    "\n",
    "print(a_2.shape, loga_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f16a7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "v_2 = critic_test([statex2, a_2])\n",
    "print(v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb6392ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 32)           384         ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 32)           128         ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate_1 (Concatenate)    (None, 64)           0           ['dense_12[0][0]',               \n",
      "                                                                  'dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 64)           4160        ['concatenate_1[0][0]']          \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 1)            65          ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze_1 (TFOpLa  None                0           ['dense_15[0][0]']               \n",
      " mbda)                                                                                            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3c971",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "66ff1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, obs_dim, a_dim, buffer_capacity=100000, batch_size=256):\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, self.a_dim))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "        self.buffer_counter += 1\n",
    "        \n",
    "    def sample(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.squeeze(tf.convert_to_tensor(self.reward_buffer[batch_indices]))\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.squeeze(tf.convert_to_tensor(self.done_buffer[batch_indices]))\n",
    "        \n",
    "        return (state_batch,\n",
    "               action_batch,\n",
    "               reward_batch,\n",
    "               next_state_batch,\n",
    "               done_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d040f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer1 = Buffer(num_states, num_actions, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "11879892",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_obs = env.reset()\n",
    "\n",
    "for i in range(buffer1.buffer_capacity):\n",
    "    a, _ = actor_test(tf.expand_dims(prev_obs, 0))\n",
    "    obs, r, d, _ = env.step(a[0])\n",
    "    \n",
    "    buffer1.record((prev_obs, a[0], r, obs, d))\n",
    "    \n",
    "    prev_obs = obs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac7dac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 1.00420107e+00, -2.06170734e+00, -1.95111438e+00,\n",
       "          3.76500399e-03, -5.14848656e-01, -9.16336247e-01,\n",
       "         -1.96551198e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "         -9.33190497e-02, -7.16679968e+00],\n",
       "        [ 1.46937006e-01, -4.19061693e+00, -2.58921517e+00,\n",
       "         -1.73384490e-02, -7.91540417e-01, -4.91787990e-02,\n",
       "          9.17468702e-02, -2.22760193e+00,  3.46213781e-01,\n",
       "         -6.39974549e-01, -4.49070893e-01],\n",
       "        [ 6.85111071e-01, -3.37399353e+00, -2.59992060e+00,\n",
       "         -1.45989893e-01, -7.95965668e-01, -6.68122644e-01,\n",
       "         -2.36964855e+00, -9.51589820e+00, -5.53315352e+00,\n",
       "         -1.17231744e+00, -1.08068486e+00],\n",
       "        [ 1.19148413e+00, -4.88641252e-01, -5.30868315e-01,\n",
       "          1.21776081e-02, -3.42407863e-02, -1.39810334e+00,\n",
       "         -5.14817619e-01, -8.11386107e+00, -7.10229354e+00,\n",
       "         -1.96788365e+00,  1.99344729e-01],\n",
       "        [ 7.41495188e-01, -3.18896483e+00, -2.51510177e+00,\n",
       "         -1.05078289e-01, -7.62563819e-01, -1.26785198e+00,\n",
       "         -2.17160774e+00, -4.78215517e+00, -1.60057275e+00,\n",
       "         -4.02463354e-01, -1.13551998e+00],\n",
       "        [ 1.46201730e-01, -4.15782269e+00, -2.61903565e+00,\n",
       "         -4.02149995e-04, -7.78537133e-01, -2.45100685e-01,\n",
       "          3.03483758e-02, -1.56713262e+00,  1.07237951e+00,\n",
       "         -7.36029154e-01,  2.79969187e-01],\n",
       "        [ 5.39328877e-01, -3.49681345e+00, -2.63263237e+00,\n",
       "         -3.93625834e-02, -7.93824802e-01, -1.71065894e+00,\n",
       "         -2.84556845e+00,  3.09569734e+00,  2.12718449e+00,\n",
       "          5.60183354e+00,  2.19133660e-01],\n",
       "        [ 1.46505637e-01, -4.17790995e+00, -2.59704870e+00,\n",
       "         -1.21531625e-02, -7.85785252e-01, -1.76480862e-01,\n",
       "          1.55370148e-02, -9.52357890e-01,  1.61313759e+00,\n",
       "         -6.54322714e-01, -1.04784837e+00],\n",
       "        [ 4.91684849e-01, -3.47709065e+00, -2.61091836e+00,\n",
       "          2.22083236e-02, -7.75650378e-01, -1.48077270e+00,\n",
       "         -3.09017866e+00, -3.17905778e-01,  1.26686441e+00,\n",
       "          1.68025262e+00,  1.08648440e+00],\n",
       "        [ 1.48074381e-01, -4.21237890e+00, -2.59200822e+00,\n",
       "         -1.96350565e-02, -7.93517960e-01,  4.50264897e-02,\n",
       "          1.92396113e-01, -3.21553094e+00, -1.04332170e+00,\n",
       "          6.59214024e-02, -8.05379495e-02]])>,\n",
       " <tf.Tensor: shape=(10, 3), dtype=float64, numpy=\n",
       " array([[-0.4517265 ,  0.97559416,  0.22264586],\n",
       "        [-0.99778175,  0.45795035,  0.23127463],\n",
       "        [-0.7357226 ,  0.95775688, -0.87474537],\n",
       "        [-0.99806422,  0.99992794, -0.9539023 ],\n",
       "        [-0.99801356, -0.96101272,  0.09290161],\n",
       "        [ 0.60715836,  0.09856411, -0.52545571],\n",
       "        [-0.90552062, -0.9980979 ,  0.86313063],\n",
       "        [-0.92361104, -0.01655217, -0.64954025],\n",
       "        [ 0.86448646, -0.18076201, -0.90984184],\n",
       "        [-0.998097  ,  0.94770014, -0.92605257]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       " array([ 0.17718799,  0.99775166,  0.28139251, -0.41943661, -0.15445237,\n",
       "         0.76920307, -0.59517727,  0.8866399 , -0.61923678,  1.06810077])>,\n",
       " <tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 9.88291790e-01, -2.18099753e+00, -2.04675970e+00,\n",
       "          3.22918571e-03, -5.70627715e-01, -7.24078989e-01,\n",
       "         -2.00914021e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "         -4.53615503e-02, -6.78705170e+00],\n",
       "        [ 1.48074381e-01, -4.21237890e+00, -2.59200822e+00,\n",
       "         -1.96350565e-02, -7.93517960e-01,  4.50264897e-02,\n",
       "          1.92396113e-01, -3.21553094e+00, -1.04332170e+00,\n",
       "          6.59214024e-02, -8.05379495e-02],\n",
       "        [ 6.66298333e-01, -3.44432806e+00, -2.64202950e+00,\n",
       "         -1.50119361e-01, -8.01311355e-01, -9.23642764e-01,\n",
       "         -2.37122277e+00, -6.80970663e+00, -3.78372092e+00,\n",
       "          9.18580896e-02, -3.36383750e-01],\n",
       "        [ 1.18706750e+00, -5.56918974e-01, -5.93537829e-01,\n",
       "          1.13436239e-03, -3.67259370e-02, -1.44381459e+00,\n",
       "         -5.92159532e-01, -9.00209064e+00, -8.55403835e+00,\n",
       "         -8.98245569e-01, -8.06878641e-01],\n",
       "        [ 7.23471302e-01, -3.23596257e+00, -2.53305167e+00,\n",
       "         -1.13544398e-01, -7.70939484e-01, -1.03718228e+00,\n",
       "         -2.32961900e+00, -6.96893241e+00, -2.88536895e+00,\n",
       "         -1.71356010e+00, -9.58794478e-01],\n",
       "        [ 1.46384668e-01, -4.16908326e+00, -2.60911940e+00,\n",
       "         -6.44454027e-03, -7.79387418e-01, -2.13976701e-01,\n",
       "          1.63691795e-02, -1.25463517e+00,  1.40178185e+00,\n",
       "         -7.76628126e-01, -4.91575313e-01],\n",
       "        [ 5.16126761e-01, -3.48021220e+00, -2.62019339e+00,\n",
       "         -4.22369588e-04, -7.86862001e-01, -1.48318993e+00,\n",
       "         -2.95717548e+00,  1.11815727e+00,  1.04275788e+00,\n",
       "          4.13221835e+00,  1.52076442e+00],\n",
       "        [ 1.46937006e-01, -4.19061693e+00, -2.58921517e+00,\n",
       "         -1.73384490e-02, -7.91540417e-01, -4.91787990e-02,\n",
       "          9.17468702e-02, -2.22760193e+00,  3.46213781e-01,\n",
       "         -6.39974549e-01, -4.49070893e-01],\n",
       "        [ 4.67184362e-01, -3.48012267e+00, -2.59587986e+00,\n",
       "          2.97666707e-02, -7.75766597e-01, -1.74694211e+00,\n",
       "         -3.04564357e+00, -3.39993243e-01,  2.48754781e+00,\n",
       "          3.60555943e-01, -1.07308750e+00],\n",
       "        [ 1.50062689e-01, -4.24045158e+00, -2.60600437e+00,\n",
       "         -1.34598373e-02, -7.93667398e-01,  9.51496646e-02,\n",
       "          3.04635911e-01, -3.80379880e+00, -2.45414831e+00,\n",
       "          1.47639534e+00,  2.91089615e-02]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer1.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956d47d",
   "metadata": {},
   "source": [
    "### Soft Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7377d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    \n",
    "    def __init__(self, env, observation_dimensions, action_dimensions, action_bound, buffer_capacity,\n",
    "                 minibatch_size=256, gamma=0.99, tau=0.95, lr=3e-4):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.a = Actor(action_dimensions, action_bound)\n",
    "        self.c_gen = Critic_Wrapper(observation_dimensions, action_dimensions)\n",
    "        self.c1 = self.c_gen.get_critic()\n",
    "        self.c2 = self.c_gen.get_critic()\n",
    "        self.tc1 = self.c_gen.get_critic()\n",
    "        self.tc2 = self.c_gen.get_critic()\n",
    "        \n",
    "        self.tc1.set_weights(self.c1.get_weights())\n",
    "        self.tc2.set_weights(self.c2.get_weights())\n",
    "\n",
    "        self.te = -np.prod(action_dimensions)\n",
    "        self.alpha = tf.Variable(0.0, dtype=tf.float32)\n",
    "        \n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c1_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c2_opt = tf.keras.optimizers.Adam(learning_rate=lr)                                                  \n",
    "        self.alpha_opt = tf.keras.optimizers.Adam(learning_rate=lr)   \n",
    "        \n",
    "        self.buffer = Buffer(observation_dimensions, action_dimensions, buffer_capacity, minibatch_size)\n",
    "        \n",
    "        self.gamma, self.tau = gamma, tau\n",
    "        \n",
    "    def train(self, max_env_step):\n",
    "        t = 0\n",
    "        epo = 0\n",
    "        while t < max_env_step:\n",
    "            p_s = self.env.reset()\n",
    "            a_losses = []\n",
    "            c1_losses = []\n",
    "            c2_losses = []\n",
    "            alpha_losses = []\n",
    "            while True:\n",
    "                a, log_a = self.a(tf.expand_dims(p_s, 0))\n",
    "                a=a[0]\n",
    "                s, r, d, _ = self.env.step(a)\n",
    "                end = 0 if d else 1\n",
    "                \n",
    "                self.buffer.record((p_s, a, r, s, end))\n",
    "                data = self.buffer.sample()\n",
    "                \n",
    "                a_loss, c1_loss, c2_loss, alpha_loss = self.update(data)\n",
    "                \n",
    "                a_losses.append(a_loss.numpy())\n",
    "                c1_losses.append(c1_loss.numpy())\n",
    "                c2_losses.append(c2_loss.numpy())\n",
    "                alpha_losses.append(alpha_loss.numpy())\n",
    "                \n",
    "                t = t+1\n",
    "                \n",
    "                if d:\n",
    "                    break\n",
    "                p_s = s\n",
    "                \n",
    "            print(\"Epoch {:04d}\".format(epo), \"Policy Avg. Loss: \", np.mean(a_losses), \n",
    "                  \", Critic 1 Avg. Loss: \",  np.mean(c1_losses), \n",
    "                  \", Critic 2 Avg. Loss: \",  np.mean(c2_losses), \n",
    "                  \", Alpha 1 Avg. Loss: \",  np.mean(alpha_losses), flush=True)\n",
    "            epo = epo+1\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def update(self, data):\n",
    "        s_b, a_b, r_b, ns_b, d_b = data\n",
    "        with tf.GradientTape() as tape_c1, tf.GradientTape() as tape_c2:\n",
    "            q1 = self.c1([s_b, a_b])\n",
    "            q2 = self.c2([s_b, a_b])\n",
    "            na, nlog_a = self.a(ns_b)\n",
    "            \n",
    "            tq1 = self.tc1([ns_b, na])\n",
    "            tq2 = self.tc2([ns_b, na])\n",
    "            \n",
    "            min_qt = tf.math.minimum(tq1,tq2)\n",
    "            \n",
    "            soft_qt = min_qt - (self.alpha*nlog_a)\n",
    "            \n",
    "            y = tf.stop_gradient(r_b+self.gamma*d_b*tf.cast(soft_qt, dtype=tf.float64))\n",
    "            \n",
    "            L_c1 = 0.5*tf.reduce_mean((y-tf.cast(q1, dtype=tf.float64))**2)\n",
    "            L_c2 = 0.5*tf.reduce_mean((y-tf.cast(q2, dtype=tf.float64))**2)\n",
    "        c1_grad = tape_c1.gradient(L_c1, self.c1.trainable_variables)\n",
    "        c2_grad = tape_c2.gradient(L_c2, self.c2.trainable_variables)\n",
    "        \n",
    "        self.c1_opt.apply_gradients(zip(c1_grad, self.c1.trainable_variables))\n",
    "        self.c2_opt.apply_gradients(zip(c2_grad, self.c2.trainable_variables))\n",
    "        \n",
    "        for (tc1w, c1w) in zip(self.tc1.variables, self.c1.variables):\n",
    "            tc1w.assign(tc1w*self.tau + c1w*(1.0-self.tau))\n",
    "        for (tc2w, c2w) in zip(self.tc2.variables, self.c2.variables):\n",
    "            tc2w.assign(tc2w*self.tau + c2w*(1.0-self.tau))\n",
    "            \n",
    "        with tf.GradientTape() as tape_a, tf.GradientTape() as tape_alpha:\n",
    "            a, log_a = self.a(s_b)\n",
    "            qa1 = self.c1([s_b, a])\n",
    "            qa2 = self.c2([s_b, a])\n",
    "            \n",
    "            soft_qa = tf.math.minimum(qa1,qa2)\n",
    "\n",
    "            L_a = -tf.reduce_mean(soft_qa-self.alpha*log_a)\n",
    "            L_alpha = -tf.reduce_mean(self.alpha*tf.stop_gradient(log_a + self.te))\n",
    "        grad_a = tape_a.gradient(L_a, self.a.trainable_variables)\n",
    "        grad_alpha = tape_alpha.gradient(L_alpha, [self.alpha])\n",
    "        self.a_opt.apply_gradients(zip(grad_a, self.a.trainable_variables))\n",
    "        self.alpha_opt.apply_gradients(zip(grad_alpha, [self.alpha]))\n",
    "        \n",
    "        return L_a, L_c1, L_c2, L_alpha\n",
    "    \n",
    "    def save_weights(self, dir_path):\n",
    "        cp = tf.train.Checkpoint(step=self.alpha)\n",
    "        self.a.save_weights(dir_path+\"/a.ckpt\")\n",
    "        print(\"Saved actor weights\", flush=True)\n",
    "        self.c1.save_weights(dir_path+\"/c1.ckpt\")\n",
    "        print(\"Saved critic 1 weights\", flush=True)\n",
    "        self.c2.save_weights(dir_path+\"/c2.ckpt\")\n",
    "        print(\"Saved critic 2 weights\", flush=True)\n",
    "        cp.save(dir_path+\"/alpha\")\n",
    "        print(\"Saved alpha weights\", flush=True)\n",
    "\n",
    "    def load_weights(self, dir_path):\n",
    "        try:\n",
    "            cp = tf.train.Checkpoint(step=self.alpha)\n",
    "            self.a.load_weights(dir_path+\"/a.ckpt\")\n",
    "            print(\"Loaded actor weights\", flush=True)\n",
    "            self.c1.load_weights(dir_path+\"/c1.ckpt\")\n",
    "            print(\"Loaded critic 1 weights\", flush=True)\n",
    "            self.c2.load_weights(dir_path+\"/c2.ckpt\")\n",
    "            print(\"Loaded critic 2 weights\", flush=True)\n",
    "            cp.restore(dir_path+\"/alpha-1\")\n",
    "            print(\"Loaded alpha weights\", flush=True)\n",
    "            self.tc1.set_weights(self.c1.get_weights())\n",
    "            self.tc2.set_weights(self.c2.get_weights())\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"ERROR: Please make sure weights are saved as .ckpt\", flush=True)\n",
    "            \n",
    "    def eval_rollout(self, problem):\n",
    "        eps_r = 0\n",
    "        eval_env = gym.make(problem)\n",
    "        eval_obs = eval_env.reset()\n",
    "\n",
    "        while True:\n",
    "            eval_env.render()\n",
    "\n",
    "            tf_eval_obs = tf.expand_dims(tf.convert_to_tensor(eval_obs), 0)\n",
    "\n",
    "            eval_a, eval_log_a = self.a(tf_eval_obs, eval_mode=True)\n",
    "\n",
    "            eval_a = eval_a[0]\n",
    "\n",
    "            eval_obs_new, eval_r, eval_d, _ = eval_env.step(eval_a)\n",
    "\n",
    "            eps_r += eval_r\n",
    "\n",
    "            if eval_d:\n",
    "                break\n",
    "                \n",
    "            eval_obs = eval_obs_new\n",
    "\n",
    "        glfw.destroy_window(eval_env.viewer.window)\n",
    "        eval_env.close()\n",
    "        print(\"rollout episodic reward: \", eps_r, flush=True)\n",
    "        \n",
    "        return eps_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "75bddba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac1 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0ba42cdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Policy Avg. Loss:  0.35969567 , Critic 1 Avg. Loss:  0.1721895517200507 , Critic 2 Avg. Loss:  0.17765680691402877 , Alpha 1 Avg. Loss:  -0.0061950134\n",
      "Epoch 0001 Policy Avg. Loss:  -0.764436 , Critic 1 Avg. Loss:  0.13202453921192908 , Critic 2 Avg. Loss:  0.15646447713014325 , Alpha 1 Avg. Loss:  -0.041318085\n",
      "Epoch 0002 Policy Avg. Loss:  -2.67977 , Critic 1 Avg. Loss:  0.2942117204913821 , Critic 2 Avg. Loss:  0.27354679253686137 , Alpha 1 Avg. Loss:  0.033842966\n",
      "Epoch 0003 Policy Avg. Loss:  -4.350079 , Critic 1 Avg. Loss:  0.42745284538773104 , Critic 2 Avg. Loss:  0.4160114465068275 , Alpha 1 Avg. Loss:  0.15859634\n",
      "Epoch 0004 Policy Avg. Loss:  -5.480203 , Critic 1 Avg. Loss:  0.6094894317218219 , Critic 2 Avg. Loss:  0.5686217444182767 , Alpha 1 Avg. Loss:  0.043561686\n",
      "Epoch 0005 Policy Avg. Loss:  -6.350147 , Critic 1 Avg. Loss:  0.7244659166415823 , Critic 2 Avg. Loss:  0.7043358277428651 , Alpha 1 Avg. Loss:  -0.249476\n",
      "Epoch 0006 Policy Avg. Loss:  -6.617394 , Critic 1 Avg. Loss:  0.8083137929925305 , Critic 2 Avg. Loss:  0.7873811570069887 , Alpha 1 Avg. Loss:  -0.57909\n",
      "Epoch 0007 Policy Avg. Loss:  -6.76987 , Critic 1 Avg. Loss:  0.8375997978182311 , Critic 2 Avg. Loss:  0.826951374304809 , Alpha 1 Avg. Loss:  -0.71540797\n",
      "Epoch 0008 Policy Avg. Loss:  -7.296078 , Critic 1 Avg. Loss:  0.8876817858074975 , Critic 2 Avg. Loss:  0.8750974638828034 , Alpha 1 Avg. Loss:  -0.678295\n",
      "Epoch 0009 Policy Avg. Loss:  -8.351017 , Critic 1 Avg. Loss:  1.104416187145269 , Critic 2 Avg. Loss:  1.0908140855394945 , Alpha 1 Avg. Loss:  -0.33587438\n",
      "Epoch 0010 Policy Avg. Loss:  -9.694911 , Critic 1 Avg. Loss:  1.205579012968929 , Critic 2 Avg. Loss:  1.1811761010791146 , Alpha 1 Avg. Loss:  -0.11535215\n",
      "Epoch 0011 Policy Avg. Loss:  -10.8869 , Critic 1 Avg. Loss:  1.5796430771554208 , Critic 2 Avg. Loss:  1.5207417723778813 , Alpha 1 Avg. Loss:  -0.13737808\n",
      "Epoch 0012 Policy Avg. Loss:  -12.167793 , Critic 1 Avg. Loss:  1.9861766923389532 , Critic 2 Avg. Loss:  1.9265419461772093 , Alpha 1 Avg. Loss:  -0.12054044\n",
      "Epoch 0013 Policy Avg. Loss:  -13.550781 , Critic 1 Avg. Loss:  2.1851748679995793 , Critic 2 Avg. Loss:  2.1003449808300956 , Alpha 1 Avg. Loss:  -0.17899981\n",
      "Epoch 0014 Policy Avg. Loss:  -14.971991 , Critic 1 Avg. Loss:  2.69493999530355 , Critic 2 Avg. Loss:  2.5796612490944155 , Alpha 1 Avg. Loss:  -0.14848731\n",
      "Epoch 0015 Policy Avg. Loss:  -16.248926 , Critic 1 Avg. Loss:  3.205926615272441 , Critic 2 Avg. Loss:  3.101481769141639 , Alpha 1 Avg. Loss:  -0.18673243\n",
      "Epoch 0016 Policy Avg. Loss:  -16.67789 , Critic 1 Avg. Loss:  4.038910523159351 , Critic 2 Avg. Loss:  3.9172761019623277 , Alpha 1 Avg. Loss:  -0.17319196\n",
      "Epoch 0017 Policy Avg. Loss:  -17.249813 , Critic 1 Avg. Loss:  4.0821000527761075 , Critic 2 Avg. Loss:  4.028648266031512 , Alpha 1 Avg. Loss:  -0.28663763\n",
      "Epoch 0018 Policy Avg. Loss:  -17.552149 , Critic 1 Avg. Loss:  4.048805836248373 , Critic 2 Avg. Loss:  4.040346928591794 , Alpha 1 Avg. Loss:  -0.26569495\n",
      "Epoch 0019 Policy Avg. Loss:  -18.185133 , Critic 1 Avg. Loss:  4.631527508184341 , Critic 2 Avg. Loss:  4.671683749950085 , Alpha 1 Avg. Loss:  -0.22482206\n",
      "Epoch 0020 Policy Avg. Loss:  -18.373432 , Critic 1 Avg. Loss:  4.899769355948241 , Critic 2 Avg. Loss:  4.81338808082195 , Alpha 1 Avg. Loss:  -0.22519508\n",
      "Epoch 0021 Policy Avg. Loss:  -18.097229 , Critic 1 Avg. Loss:  4.95158800275197 , Critic 2 Avg. Loss:  4.91939431395128 , Alpha 1 Avg. Loss:  -0.18194017\n",
      "Epoch 0022 Policy Avg. Loss:  -18.401997 , Critic 1 Avg. Loss:  5.30642206034568 , Critic 2 Avg. Loss:  5.2221387396524594 , Alpha 1 Avg. Loss:  -0.135241\n",
      "Epoch 0023 Policy Avg. Loss:  -18.455935 , Critic 1 Avg. Loss:  4.385031239908829 , Critic 2 Avg. Loss:  4.306176876418034 , Alpha 1 Avg. Loss:  -0.22956212\n",
      "Epoch 0024 Policy Avg. Loss:  -18.314753 , Critic 1 Avg. Loss:  5.15643813004439 , Critic 2 Avg. Loss:  5.000780604590296 , Alpha 1 Avg. Loss:  -0.20817448\n",
      "Epoch 0025 Policy Avg. Loss:  -18.018198 , Critic 1 Avg. Loss:  4.278564316120383 , Critic 2 Avg. Loss:  4.117944131259995 , Alpha 1 Avg. Loss:  -0.31873956\n",
      "Epoch 0026 Policy Avg. Loss:  -17.822544 , Critic 1 Avg. Loss:  4.368377880302405 , Critic 2 Avg. Loss:  4.172553127986417 , Alpha 1 Avg. Loss:  -0.1574305\n",
      "Epoch 0027 Policy Avg. Loss:  -18.223337 , Critic 1 Avg. Loss:  4.019537616201846 , Critic 2 Avg. Loss:  3.766692915062009 , Alpha 1 Avg. Loss:  -0.25064015\n",
      "Epoch 0028 Policy Avg. Loss:  -19.301344 , Critic 1 Avg. Loss:  4.498125036905597 , Critic 2 Avg. Loss:  4.148443198566474 , Alpha 1 Avg. Loss:  -0.3817317\n",
      "Epoch 0029 Policy Avg. Loss:  -21.931719 , Critic 1 Avg. Loss:  5.9265460493933455 , Critic 2 Avg. Loss:  5.512775838298371 , Alpha 1 Avg. Loss:  -0.40422887\n",
      "Epoch 0030 Policy Avg. Loss:  -24.19646 , Critic 1 Avg. Loss:  6.162817320639071 , Critic 2 Avg. Loss:  5.901250998733871 , Alpha 1 Avg. Loss:  -0.37088266\n",
      "Epoch 0031 Policy Avg. Loss:  -24.898785 , Critic 1 Avg. Loss:  7.40298766377643 , Critic 2 Avg. Loss:  7.221834378721685 , Alpha 1 Avg. Loss:  -0.42968312\n",
      "Epoch 0032 Policy Avg. Loss:  -24.712593 , Critic 1 Avg. Loss:  7.729596628799708 , Critic 2 Avg. Loss:  7.575820107619899 , Alpha 1 Avg. Loss:  -0.31538084\n",
      "Epoch 0033 Policy Avg. Loss:  -25.558523 , Critic 1 Avg. Loss:  7.165595874748704 , Critic 2 Avg. Loss:  6.994305069950877 , Alpha 1 Avg. Loss:  -0.49211618\n",
      "Epoch 0034 Policy Avg. Loss:  -24.674551 , Critic 1 Avg. Loss:  9.471703362465346 , Critic 2 Avg. Loss:  9.417908829098437 , Alpha 1 Avg. Loss:  -0.7477607\n",
      "Epoch 0035 Policy Avg. Loss:  -24.306702 , Critic 1 Avg. Loss:  9.03931731886327 , Critic 2 Avg. Loss:  9.063943575392136 , Alpha 1 Avg. Loss:  -0.43214175\n",
      "Epoch 0036 Policy Avg. Loss:  -23.887632 , Critic 1 Avg. Loss:  8.884029942332253 , Critic 2 Avg. Loss:  8.951128952251151 , Alpha 1 Avg. Loss:  -0.79909056\n",
      "Epoch 0037 Policy Avg. Loss:  -22.8411 , Critic 1 Avg. Loss:  7.633750678517487 , Critic 2 Avg. Loss:  7.688717481563721 , Alpha 1 Avg. Loss:  -0.39256665\n",
      "Epoch 0038 Policy Avg. Loss:  -22.334873 , Critic 1 Avg. Loss:  7.66698650006646 , Critic 2 Avg. Loss:  7.593522355191779 , Alpha 1 Avg. Loss:  -0.44753852\n",
      "Epoch 0039 Policy Avg. Loss:  -20.955624 , Critic 1 Avg. Loss:  6.09674996986076 , Critic 2 Avg. Loss:  5.99266875770476 , Alpha 1 Avg. Loss:  -0.3177763\n",
      "Epoch 0040 Policy Avg. Loss:  -20.323378 , Critic 1 Avg. Loss:  5.887071268410748 , Critic 2 Avg. Loss:  5.780791860377598 , Alpha 1 Avg. Loss:  -0.35094494\n",
      "Epoch 0041 Policy Avg. Loss:  -19.36202 , Critic 1 Avg. Loss:  5.40756073783096 , Critic 2 Avg. Loss:  5.241444725746851 , Alpha 1 Avg. Loss:  -0.39001805\n",
      "Epoch 0042 Policy Avg. Loss:  -19.768023 , Critic 1 Avg. Loss:  4.828484634416809 , Critic 2 Avg. Loss:  4.768657486448322 , Alpha 1 Avg. Loss:  -0.6933462\n",
      "Epoch 0043 Policy Avg. Loss:  -19.838125 , Critic 1 Avg. Loss:  4.5267920100176005 , Critic 2 Avg. Loss:  4.420324315439502 , Alpha 1 Avg. Loss:  -0.6576446\n",
      "Epoch 0044 Policy Avg. Loss:  -20.062767 , Critic 1 Avg. Loss:  4.544216004454146 , Critic 2 Avg. Loss:  4.473179232037327 , Alpha 1 Avg. Loss:  -0.39831617\n",
      "Epoch 0045 Policy Avg. Loss:  -19.782074 , Critic 1 Avg. Loss:  4.307700271206677 , Critic 2 Avg. Loss:  4.203093079318211 , Alpha 1 Avg. Loss:  -0.37812507\n",
      "Epoch 0046 Policy Avg. Loss:  -19.737368 , Critic 1 Avg. Loss:  4.15471294033348 , Critic 2 Avg. Loss:  4.0842336047207946 , Alpha 1 Avg. Loss:  -0.41282502\n"
     ]
    }
   ],
   "source": [
    "sac1.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "94e70621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved actor weights\n",
      "Saved critic 1 weights\n",
      "Saved critic 2 weights\n",
      "Saved alpha weights\n"
     ]
    }
   ],
   "source": [
    "sac1.save_weights(\"/home/tony/rl_models/MA2C/sac/devel/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d26664a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.14045984>\n"
     ]
    }
   ],
   "source": [
    "print(sac1.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "909c1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac2 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ba100f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor weights\n",
      "Loaded critic 1 weights\n",
      "Loaded critic 2 weights\n",
      "Loaded alpha weights\n"
     ]
    }
   ],
   "source": [
    "sac2.load_weights(\"/home/tony/rl_models/MA2C/sac/devel/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97bceeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.14045984>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac2.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "17c45be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Policy Avg. Loss:  -23.318287 , Critic 1 Avg. Loss:  0.838302681979099 , Critic 2 Avg. Loss:  0.898270274492932 , Alpha 1 Avg. Loss:  -0.4271607\n",
      "Epoch 0001 Policy Avg. Loss:  -20.082031 , Critic 1 Avg. Loss:  3.106233222639545 , Critic 2 Avg. Loss:  3.004582268098048 , Alpha 1 Avg. Loss:  -0.17873415\n",
      "Epoch 0002 Policy Avg. Loss:  -19.528559 , Critic 1 Avg. Loss:  3.3412309989369233 , Critic 2 Avg. Loss:  3.4306676653636536 , Alpha 1 Avg. Loss:  -0.22907627\n",
      "Epoch 0003 Policy Avg. Loss:  -19.577868 , Critic 1 Avg. Loss:  3.645833821023125 , Critic 2 Avg. Loss:  3.646975291057017 , Alpha 1 Avg. Loss:  -0.042650133\n",
      "Epoch 0004 Policy Avg. Loss:  -19.931627 , Critic 1 Avg. Loss:  4.450653939904594 , Critic 2 Avg. Loss:  4.445639566240925 , Alpha 1 Avg. Loss:  -0.0013434519\n",
      "Epoch 0005 Policy Avg. Loss:  -20.515638 , Critic 1 Avg. Loss:  5.364437359725915 , Critic 2 Avg. Loss:  5.233526948583891 , Alpha 1 Avg. Loss:  0.055890635\n",
      "Epoch 0006 Policy Avg. Loss:  -21.156134 , Critic 1 Avg. Loss:  5.854301683774878 , Critic 2 Avg. Loss:  5.630767059955999 , Alpha 1 Avg. Loss:  0.048510056\n",
      "Epoch 0007 Policy Avg. Loss:  -21.845697 , Critic 1 Avg. Loss:  6.636643805331402 , Critic 2 Avg. Loss:  6.223705428701078 , Alpha 1 Avg. Loss:  -0.0031635987\n",
      "Epoch 0008 Policy Avg. Loss:  -22.676805 , Critic 1 Avg. Loss:  5.957556372153144 , Critic 2 Avg. Loss:  5.6292342795358845 , Alpha 1 Avg. Loss:  -0.13013932\n",
      "Epoch 0009 Policy Avg. Loss:  -24.708176 , Critic 1 Avg. Loss:  6.85878630517276 , Critic 2 Avg. Loss:  6.5118722544214185 , Alpha 1 Avg. Loss:  -0.11548943\n",
      "Epoch 0010 Policy Avg. Loss:  -29.59327 , Critic 1 Avg. Loss:  7.740093184144021 , Critic 2 Avg. Loss:  7.81359910289188 , Alpha 1 Avg. Loss:  -0.24614546\n",
      "Epoch 0011 Policy Avg. Loss:  -31.973955 , Critic 1 Avg. Loss:  9.213016520126597 , Critic 2 Avg. Loss:  8.997868076801092 , Alpha 1 Avg. Loss:  -0.008608904\n"
     ]
    }
   ],
   "source": [
    "sac2.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98f816b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "rollout episodic reward:  32.67973995901737\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32.67973995901737"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac2.eval_rollout(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e185df41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
