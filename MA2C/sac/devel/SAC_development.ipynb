{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc7ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import regularizers\n",
    "import glfw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cca3c",
   "metadata": {},
   "source": [
    "### Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e91a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"Hopper-v3\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafe2ab",
   "metadata": {},
   "source": [
    "### Actor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aeb90",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279be0e3",
   "metadata": {},
   "source": [
    "Same as PPO actor. Gaussian policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e15e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10\n",
    "\n",
    "class Actor(Model):\n",
    "\n",
    "    def __init__(self, action_dimensions, action_bound):\n",
    "        super().__init__()\n",
    "        self.action_dim, self.upper_bound = action_dimensions, action_bound\n",
    "        self.sample_dist = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(self.action_dim),\n",
    "                                                                    scale_diag=tf.ones(self.action_dim))\n",
    "        self.dense1_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.mean_layer = layers.Dense(self.action_dim)\n",
    "        self.stdev_layer = layers.Dense(self.action_dim)\n",
    "\n",
    "    def call(self, state, eval_mode=False):\n",
    "\n",
    "        a1 = self.dense1_layer(state)\n",
    "        a2 = self.dense2_layer(a1)\n",
    "        mu = self.mean_layer(a2)\n",
    "\n",
    "        log_sigma = self.stdev_layer(a2)\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        sigma = tf.clip_by_value(sigma, EPSILON, 2.718)\n",
    "\n",
    "        dist = tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "        \n",
    "        if eval_mode:\n",
    "            action_ = mu\n",
    "        else:\n",
    "            action_ = tf.math.add(mu, tf.math.multiply(sigma, tf.expand_dims(self.sample_dist.sample(), 0)))\n",
    " \n",
    "        action = tf.tanh(action_)\n",
    "\n",
    "        log_pi_ = dist.log_prob(action_)     \n",
    "        log_pi = log_pi_ - tf.reduce_sum(tf.math.log(tf.clip_by_value(1 - action**2, EPSILON, 1.0)), axis=1)\n",
    "        \n",
    "        return action*self.upper_bound, log_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d3f74b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 10:42:08.726250: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:08.766254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:08.766547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:08.768079: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-31 10:42:08.769474: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:08.769763: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:08.770007: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:09.407123: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:09.407237: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:09.407317: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-31 10:42:09.407392: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5890 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "actor_test = Actor(num_actions, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d7cab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[-0.5842715   0.18573198  0.61166394]], shape=(1, 3), dtype=float32) tf.Tensor([-2.6891878], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-31 10:42:12.064486: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs\n",
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "tf_obs\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "print(a_test, log_a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "116539f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               multiple                  768       \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  195       \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,318\n",
      "Trainable params: 5,318\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fae85",
   "metadata": {},
   "source": [
    "### Critic Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a403d18b",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347d502",
   "metadata": {},
   "source": [
    "Different from PPO, critic evaluate state-action pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Wrapper():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.s_dim=state_dim\n",
    "        self.a_dim=action_dim\n",
    "        \n",
    "    def get_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(self.s_dim))\n",
    "        state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(self.a_dim))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "        out = layers.Dense(64, activation=\"relu\")(concat)\n",
    "        outputs = tf.squeeze(layers.Dense(1)(out))\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dadd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_gen = Critic_Wrapper(num_states, num_actions)\n",
    "critic_test = critic_gen.get_critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65aced4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25402812e+00,  2.99453746e-03, -9.47262251e-04,  1.20319874e-03,\n",
       "       -3.29992226e-03, -6.56976199e-04,  4.02123600e-03, -7.23289060e-05,\n",
       "       -2.53804638e-03, -3.73262170e-03,  2.78503338e-03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c101830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11), dtype=float64, numpy=\n",
       "array([[ 1.25402812e+00,  2.99453746e-03, -9.47262251e-04,\n",
       "         1.20319874e-03, -3.29992226e-03, -6.56976199e-04,\n",
       "         4.02123600e-03, -7.23289060e-05, -2.53804638e-03,\n",
       "        -3.73262170e-03,  2.78503338e-03]])>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "tf_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa334304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.1329266>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_test = critic_test([tf_obs, a_test])\n",
    "v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a2cfe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11)\n"
     ]
    }
   ],
   "source": [
    "obs_new, _, _, _ = env.step(a_test[0])\n",
    "tf_obs_new = tf.expand_dims(obs_new, 0)\n",
    "statex2 = tf.convert_to_tensor([obs, obs_new])\n",
    "print(statex2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df584535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2,)\n"
     ]
    }
   ],
   "source": [
    "a_2, loga_2 = actor_test(statex2)\n",
    "\n",
    "print(a_2.shape, loga_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f16a7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "v_2 = critic_test([statex2, a_2])\n",
    "print(v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb6392ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           384         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           128         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64)           0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           4160        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            65          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  None                0           ['dense_7[0][0]']                \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3c971",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66ff1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, obs_dim, a_dim, buffer_capacity=100000, batch_size=256):\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, self.a_dim))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "        self.buffer_counter += 1\n",
    "        \n",
    "    def sample(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.squeeze(tf.convert_to_tensor(self.reward_buffer[batch_indices]))\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.squeeze(tf.convert_to_tensor(self.done_buffer[batch_indices]))\n",
    "        \n",
    "        return (state_batch,\n",
    "               action_batch,\n",
    "               reward_batch,\n",
    "               next_state_batch,\n",
    "               done_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9d040f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer1 = Buffer(num_states, num_actions, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11879892",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_obs = env.reset()\n",
    "\n",
    "for i in range(buffer1.buffer_capacity):\n",
    "    a, _ = actor_test(tf.expand_dims(prev_obs, 0))\n",
    "    obs, r, d, _ = env.step(a[0])\n",
    "    \n",
    "    buffer1.record((prev_obs, a[0], r, obs, d))\n",
    "    \n",
    "    prev_obs = obs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac7dac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 8.69372715e-01, -3.00963174e+00, -2.79845387e+00,\n",
       "         -3.27863727e-01, -9.15627053e-02, -9.24249484e-01,\n",
       "         -6.70892139e-01,  1.49432012e+00,  2.35849928e+00,\n",
       "          1.41801151e+00, -3.61775287e+00],\n",
       "        [ 8.38194016e-01, -2.89084495e+00, -2.63417902e+00,\n",
       "         -2.79396284e-01, -1.86531476e-01, -1.47238248e+00,\n",
       "         -2.69098563e-01,  9.60786565e-01,  2.66084400e+00,\n",
       "         -4.68988572e-01, -8.85415588e-01],\n",
       "        [ 4.81000625e-01, -3.49818132e+00, -2.62029831e+00,\n",
       "          1.42552060e-03, -8.10823940e-01, -1.67721853e+00,\n",
       "         -1.98403877e+00, -3.43661643e+00, -3.38218514e-02,\n",
       "          2.75242251e-02,  1.93208188e-01],\n",
       "        [ 1.01439020e+00, -1.79497629e+00, -1.75549328e+00,\n",
       "         -2.90742332e-01,  9.06561351e-02,  3.23381203e-01,\n",
       "         -3.57082529e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "         -2.08031026e+00,  1.75709912e+00],\n",
       "        [ 3.82470328e-01, -3.67566611e+00, -2.62110494e+00,\n",
       "          1.89651496e-03, -7.93724054e-01, -1.59753362e+00,\n",
       "         -2.20795426e+00, -3.93153076e+00, -8.89862411e-03,\n",
       "          1.35795535e-02,  2.18531143e-01],\n",
       "        [ 6.07462518e-01, -3.29224707e+00, -2.62644755e+00,\n",
       "          7.47612198e-03, -6.66886201e-01, -1.75956735e+00,\n",
       "         -1.83630085e+00, -2.34186770e+00,  6.66265252e-01,\n",
       "         -1.87178516e-01, -3.15399075e+00],\n",
       "        [ 7.71985094e-01, -3.00823275e+00, -2.62192897e+00,\n",
       "         -1.83861485e-01, -2.46735293e-01, -1.84610841e+00,\n",
       "         -5.81624582e-01, -7.57977782e-01,  8.64874168e-02,\n",
       "          2.56503136e+00, -2.98503504e+00],\n",
       "        [ 8.63405211e-01, -2.99574201e+00, -2.77491208e+00,\n",
       "         -3.22058930e-01, -1.14504721e-01, -1.02116214e+00,\n",
       "         -8.25732509e-01,  1.76161304e+00,  3.32190686e+00,\n",
       "          4.24734357e-02, -2.12135955e+00],\n",
       "        [ 1.20290856e+00, -2.24378917e-01, -2.07482729e-01,\n",
       "         -1.09327877e-01,  1.57469242e-01, -1.77287624e-01,\n",
       "         -6.59214092e-01, -4.93842864e+00, -5.95070790e+00,\n",
       "          3.68359903e-01, -8.24710451e-01],\n",
       "        [ 3.64500821e-01, -3.70731374e+00, -2.62111716e+00,\n",
       "          2.01046342e-03, -7.92197439e-01, -1.55632531e+00,\n",
       "         -2.28539077e+00, -3.98145637e+00,  4.16656469e-03,\n",
       "          1.44772654e-02,  1.65338783e-01]])>,\n",
       " <tf.Tensor: shape=(10, 3), dtype=float64, numpy=\n",
       " array([[-0.92029232, -0.90303814,  0.9820255 ],\n",
       "        [-0.97740012, -0.0265322 ,  0.89603359],\n",
       "        [-0.89767563,  0.65976524,  0.14121071],\n",
       "        [-0.97907501, -0.00844876, -0.98096758],\n",
       "        [-0.70473903,  0.76210272, -0.6347267 ],\n",
       "        [-0.89060289,  0.89258271,  0.67663258],\n",
       "        [-0.53080153,  0.98976046,  0.61741447],\n",
       "        [-0.90457207,  0.73292798,  0.21395314],\n",
       "        [-0.82157725,  0.81873262, -0.99704248],\n",
       "        [-0.94381142,  0.90398592,  0.56234205]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       " array([ 0.01185175, -0.38576449, -0.68271649,  1.5272393 , -0.57912907,\n",
       "        -0.7304788 , -0.84214361, -0.02767582,  0.84012926, -0.54311502])>,\n",
       " <tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 8.63405211e-01, -2.99574201e+00, -2.77491208e+00,\n",
       "         -3.22058930e-01, -1.14504721e-01, -1.02116214e+00,\n",
       "         -8.25732509e-01,  1.76161304e+00,  3.32190686e+00,\n",
       "          4.24734357e-02, -2.12135955e+00],\n",
       "        [ 8.35604542e-01, -2.88833734e+00, -2.61835528e+00,\n",
       "         -2.83257097e-01, -1.88021016e-01, -1.29521532e+00,\n",
       "         -3.86044007e-01, -3.00569104e-01,  1.32973619e+00,\n",
       "         -4.97776120e-01,  4.88844344e-01],\n",
       "        [ 4.65153799e-01, -3.52600186e+00, -2.62053870e+00,\n",
       "          1.60790861e-03, -8.08243360e-01, -1.68483826e+00,\n",
       "         -1.98079748e+00, -3.52069836e+00, -2.64535968e-02,\n",
       "          1.88149231e-02,  4.14381935e-01],\n",
       "        [ 9.85327281e-01, -1.98365890e+00, -1.92711816e+00,\n",
       "         -3.07533330e-01,  9.87643459e-02,  7.45030226e-01,\n",
       "         -3.67140095e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "         -2.12690589e+00,  2.72554600e-01],\n",
       "        [ 3.64500821e-01, -3.70731374e+00, -2.62111716e+00,\n",
       "          2.01046342e-03, -7.92197439e-01, -1.55632531e+00,\n",
       "         -2.28539077e+00, -3.98145637e+00,  4.16656469e-03,\n",
       "          1.44772654e-02,  1.65338783e-01],\n",
       "        [ 5.92764328e-01, -3.31246661e+00, -2.62255097e+00,\n",
       "          6.18671715e-03, -6.89149497e-01, -1.70320422e+00,\n",
       "         -1.83457327e+00, -2.68229400e+00,  3.38373042e-01,\n",
       "         -1.37684363e-01, -2.42085034e+00],\n",
       "        [ 7.67335698e-01, -3.01043602e+00, -2.62135238e+00,\n",
       "         -1.57876148e-01, -2.66984245e-01, -1.83371857e+00,\n",
       "         -5.84158330e-01,  2.03151026e-01,  5.94174105e-02,\n",
       "          3.93124098e+00, -2.08173484e+00],\n",
       "        [ 8.56701823e-01, -2.97818951e+00, -2.74765613e+00,\n",
       "         -3.17568514e-01, -1.30276267e-01, -1.01920580e+00,\n",
       "         -8.51626563e-01,  2.52042971e+00,  3.39282914e+00,\n",
       "          1.08230059e+00, -1.82286429e+00],\n",
       "        [ 1.19763133e+00, -2.66253608e-01, -2.60156058e-01,\n",
       "         -1.01646131e-01,  1.44252325e-01, -1.38167128e-01,\n",
       "         -6.69238554e-01, -5.52506000e+00, -7.21470299e+00,\n",
       "          1.55529481e+00, -2.46698924e+00],\n",
       "        [ 3.46067902e-01, -3.73921718e+00, -2.62111218e+00,\n",
       "          2.14346070e-03, -7.88352186e-01, -1.52735751e+00,\n",
       "         -2.32126400e+00, -3.99561986e+00, -2.10824278e-03,\n",
       "          1.78047945e-02,  7.89178361e-01]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer1.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956d47d",
   "metadata": {},
   "source": [
    "### Soft Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "7377d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    \n",
    "    def __init__(self, env, observation_dimensions, action_dimensions, action_bound, buffer_capacity,\n",
    "                 minibatch_size=256, gamma=0.99, tau=0.95, lr=3e-4):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.a = Actor(action_dimensions, action_bound)\n",
    "        self.c_gen = Critic_Wrapper(observation_dimensions, action_dimensions)\n",
    "        self.c1 = self.c_gen.get_critic()\n",
    "        self.c2 = self.c_gen.get_critic()\n",
    "        self.tc1 = self.c_gen.get_critic()\n",
    "        self.tc2 = self.c_gen.get_critic()\n",
    "        \n",
    "        self.tc1.set_weights(self.c1.get_weights())\n",
    "        self.tc2.set_weights(self.c2.get_weights())\n",
    "\n",
    "        self.te = -np.prod(action_dimensions)\n",
    "        self.alpha = tf.Variable(0.0, dtype=tf.float32)\n",
    "        \n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c1_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c2_opt = tf.keras.optimizers.Adam(learning_rate=lr)                                                  \n",
    "        self.alpha_opt = tf.keras.optimizers.Adam(learning_rate=lr)   \n",
    "        \n",
    "        self.buffer = Buffer(observation_dimensions, action_dimensions, buffer_capacity, minibatch_size)\n",
    "        \n",
    "        self.gamma, self.tau = gamma, tau\n",
    "        \n",
    "    def train(self, max_env_step):\n",
    "        t = 0\n",
    "        epo = 0\n",
    "        while t < max_env_step:\n",
    "            p_s = self.env.reset()\n",
    "            a_losses = []\n",
    "            c1_losses = []\n",
    "            c2_losses = []\n",
    "            alpha_losses = []\n",
    "            while True:\n",
    "                a, log_a = self.a(tf.expand_dims(p_s, 0))\n",
    "                a=a[0]\n",
    "                s, r, d, _ = self.env.step(a)\n",
    "                end = 0 if d else 1\n",
    "                \n",
    "                self.buffer.record((p_s, a, r, s, end))\n",
    "                data = self.buffer.sample()\n",
    "                \n",
    "                a_loss, c1_loss, c2_loss, alpha_loss = self.update(data)\n",
    "                \n",
    "                a_losses.append(a_loss.numpy())\n",
    "                c1_losses.append(c1_loss.numpy())\n",
    "                c2_losses.append(c2_loss.numpy())\n",
    "                alpha_losses.append(alpha_loss.numpy())\n",
    "                \n",
    "                t = t+1\n",
    "                \n",
    "                if d:\n",
    "                    break\n",
    "                p_s = s\n",
    "                \n",
    "            print(\"Epoch {:04d}\".format(epo), \"Policy Avg. Loss: \", np.mean(a_losses), \n",
    "                  \", Critic 1 Avg. Loss: \",  np.mean(c1_losses), \n",
    "                  \", Critic 2 Avg. Loss: \",  np.mean(c2_losses), \n",
    "                  \", Alpha 1 Avg. Loss: \",  np.mean(alpha_losses), flush=True)\n",
    "            epo = epo+1\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def update(self, data):\n",
    "        s_b, a_b, r_b, ns_b, d_b = data\n",
    "        with tf.GradientTape() as tape_c1, tf.GradientTape() as tape_c2:\n",
    "            q1 = self.c1([s_b, a_b])\n",
    "            q2 = self.c2([s_b, a_b])\n",
    "            na, nlog_a = self.a(ns_b)\n",
    "            \n",
    "            tq1 = self.tc1([ns_b, na])\n",
    "            tq2 = self.tc2([ns_b, na])\n",
    "            \n",
    "            min_qt = tf.math.minimum(tq1,tq2)\n",
    "            \n",
    "            soft_qt = min_qt - (self.alpha*nlog_a)\n",
    "            \n",
    "            y = tf.stop_gradient(r_b+self.gamma*d_b*tf.cast(soft_qt, dtype=tf.float64))\n",
    "            \n",
    "            L_c1 = 0.5*tf.reduce_mean((y-tf.cast(q1, dtype=tf.float64))**2)\n",
    "            L_c2 = 0.5*tf.reduce_mean((y-tf.cast(q2, dtype=tf.float64))**2)\n",
    "        c1_grad = tape_c1.gradient(L_c1, self.c1.trainable_variables)\n",
    "        c2_grad = tape_c2.gradient(L_c2, self.c2.trainable_variables)\n",
    "        \n",
    "        self.c1_opt.apply_gradients(zip(c1_grad, self.c1.trainable_variables))\n",
    "        self.c2_opt.apply_gradients(zip(c2_grad, self.c2.trainable_variables))\n",
    "        \n",
    "        for (tc1w, c1w) in zip(self.tc1.variables, self.c1.variables):\n",
    "            tc1w.assign(tc1w*self.tau + c1w*(1.0-self.tau))\n",
    "        for (tc2w, c2w) in zip(self.tc2.variables, self.c2.variables):\n",
    "            tc2w.assign(tc2w*self.tau + c2w*(1.0-self.tau))\n",
    "            \n",
    "        with tf.GradientTape() as tape_a, tf.GradientTape() as tape_alpha:\n",
    "            a, log_a = self.a(s_b)\n",
    "            qa1 = self.c1([s_b, a])\n",
    "            qa2 = self.c2([s_b, a])\n",
    "            \n",
    "            soft_qa = tf.reduce_mean([qa1,qa2], axis=0)\n",
    "            \n",
    "            L_a = -tf.reduce_mean(soft_qa-self.alpha*log_a)\n",
    "            L_alpha = -tf.reduce_mean(self.alpha*tf.stop_gradient(log_a + self.te))\n",
    "        grad_a = tape_a.gradient(L_a, self.a.trainable_variables)\n",
    "        grad_alpha = tape_alpha.gradient(L_alpha, [self.alpha])\n",
    "        self.a_opt.apply_gradients(zip(grad_a, self.a.trainable_variables))\n",
    "        self.alpha_opt.apply_gradients(zip(grad_alpha, [self.alpha]))\n",
    "        \n",
    "        return L_a, L_c1, L_c2, L_alpha\n",
    "    \n",
    "    def save_weights(self, dir_path):\n",
    "        cp = tf.train.Checkpoint(step=self.alpha)\n",
    "        self.a.save_weights(dir_path+\"/a.ckpt\")\n",
    "        print(\"Saved actor weights\", flush=True)\n",
    "        self.c1.save_weights(dir_path+\"/c1.ckpt\")\n",
    "        print(\"Saved critic 1 weights\", flush=True)\n",
    "        self.c2.save_weights(dir_path+\"/c2.ckpt\")\n",
    "        print(\"Saved critic 2 weights\", flush=True)\n",
    "        cp.save(dir_path+\"/alpha\")\n",
    "        print(\"Saved alpha weights\", flush=True)\n",
    "\n",
    "    def load_weights(self, dir_path):\n",
    "        try:\n",
    "            cp = tf.train.Checkpoint(step=self.alpha)\n",
    "            self.a.load_weights(dir_path+\"/a.ckpt\")\n",
    "            print(\"Loaded actor weights\", flush=True)\n",
    "            self.c1.load_weights(dir_path+\"/c1.ckpt\")\n",
    "            print(\"Loaded critic 1 weights\", flush=True)\n",
    "            self.c2.load_weights(dir_path+\"/c2.ckpt\")\n",
    "            print(\"Loaded critic 2 weights\", flush=True)\n",
    "            cp.restore(dir_path+\"/alpha-1\")\n",
    "            print(\"Loaded alpha weights\", flush=True)\n",
    "            self.tc1.set_weights(self.c1.get_weights())\n",
    "            self.tc2.set_weights(self.c2.get_weights())\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"ERROR: Please make sure weights are saved as .ckpt\", flush=True)\n",
    "            \n",
    "    def eval_rollout(self, problem):\n",
    "        eps_r = 0\n",
    "        eval_env = gym.make(problem)\n",
    "        eval_obs = eval_env.reset()\n",
    "\n",
    "        while True:\n",
    "            eval_env.render()\n",
    "\n",
    "            tf_eval_obs = tf.expand_dims(tf.convert_to_tensor(eval_obs), 0)\n",
    "\n",
    "            eval_a, eval_log_a = self.a(tf_eval_obs, eval_mode=True)\n",
    "\n",
    "            eval_a = eval_a[0]\n",
    "\n",
    "            eval_obs_new, eval_r, eval_d, _ = eval_env.step(eval_a)\n",
    "\n",
    "            eps_r += eval_r\n",
    "\n",
    "            if eval_d:\n",
    "                break\n",
    "                \n",
    "            eval_obs = eval_obs_new\n",
    "\n",
    "        glfw.destroy_window(eval_env.viewer.window)\n",
    "        eval_env.close()\n",
    "        print(\"rollout episodic reward: \", eps_r, flush=True)\n",
    "        \n",
    "        return eps_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "75bddba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac1 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "0ba42cdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Policy Avg. Loss:  -0.07668959 , Critic 1 Avg. Loss:  0.28935144822999087 , Critic 2 Avg. Loss:  0.2945518708156598 , Alpha 1 Avg. Loss:  -0.013086394\n",
      "Epoch 0001 Policy Avg. Loss:  -0.37098062 , Critic 1 Avg. Loss:  0.1599065292218368 , Critic 2 Avg. Loss:  0.216103861002796 , Alpha 1 Avg. Loss:  -0.035733745\n",
      "Epoch 0002 Policy Avg. Loss:  -0.4911875 , Critic 1 Avg. Loss:  0.19752692609722128 , Critic 2 Avg. Loss:  0.2330697809229016 , Alpha 1 Avg. Loss:  -0.043364763\n",
      "Epoch 0003 Policy Avg. Loss:  -0.80982494 , Critic 1 Avg. Loss:  0.21432545460197494 , Critic 2 Avg. Loss:  0.2594726607585095 , Alpha 1 Avg. Loss:  -0.017548447\n",
      "Epoch 0004 Policy Avg. Loss:  -0.966272 , Critic 1 Avg. Loss:  0.2766884804454125 , Critic 2 Avg. Loss:  0.36585947620109854 , Alpha 1 Avg. Loss:  0.003943726\n",
      "Epoch 0005 Policy Avg. Loss:  -1.1295338 , Critic 1 Avg. Loss:  0.3134598160795796 , Critic 2 Avg. Loss:  0.3607355324398458 , Alpha 1 Avg. Loss:  0.06653551\n",
      "Epoch 0006 Policy Avg. Loss:  -1.3236784 , Critic 1 Avg. Loss:  0.35135487921145525 , Critic 2 Avg. Loss:  0.38738422066596245 , Alpha 1 Avg. Loss:  0.098396525\n",
      "Epoch 0007 Policy Avg. Loss:  -1.5495354 , Critic 1 Avg. Loss:  0.4077484254968456 , Critic 2 Avg. Loss:  0.4263687698479096 , Alpha 1 Avg. Loss:  0.13005295\n",
      "Epoch 0008 Policy Avg. Loss:  -1.6547618 , Critic 1 Avg. Loss:  0.3989264515312907 , Critic 2 Avg. Loss:  0.42658910029561004 , Alpha 1 Avg. Loss:  0.1071952\n",
      "Epoch 0009 Policy Avg. Loss:  -1.705477 , Critic 1 Avg. Loss:  0.3469337794909531 , Critic 2 Avg. Loss:  0.373668510597007 , Alpha 1 Avg. Loss:  0.054721765\n",
      "Epoch 0010 Policy Avg. Loss:  -1.628127 , Critic 1 Avg. Loss:  0.29036494847923844 , Critic 2 Avg. Loss:  0.34015160787225607 , Alpha 1 Avg. Loss:  -0.033170737\n",
      "Epoch 0011 Policy Avg. Loss:  -1.4419534 , Critic 1 Avg. Loss:  0.22473831093852095 , Critic 2 Avg. Loss:  0.2894507292011758 , Alpha 1 Avg. Loss:  -0.13473852\n",
      "Epoch 0012 Policy Avg. Loss:  -1.216556 , Critic 1 Avg. Loss:  0.15819323415919936 , Critic 2 Avg. Loss:  0.24182310157908415 , Alpha 1 Avg. Loss:  -0.25197843\n",
      "Epoch 0013 Policy Avg. Loss:  -0.9560371 , Critic 1 Avg. Loss:  0.14471876215723425 , Critic 2 Avg. Loss:  0.2193879916024969 , Alpha 1 Avg. Loss:  -0.33958006\n",
      "Epoch 0014 Policy Avg. Loss:  -0.6987981 , Critic 1 Avg. Loss:  0.15700099832739428 , Critic 2 Avg. Loss:  0.21842773406401456 , Alpha 1 Avg. Loss:  -0.4023671\n",
      "Epoch 0015 Policy Avg. Loss:  -0.45808625 , Critic 1 Avg. Loss:  0.18703572074079494 , Critic 2 Avg. Loss:  0.22702231712753523 , Alpha 1 Avg. Loss:  -0.4423489\n",
      "Epoch 0016 Policy Avg. Loss:  -0.26538053 , Critic 1 Avg. Loss:  0.23954695278978788 , Critic 2 Avg. Loss:  0.25214814955287534 , Alpha 1 Avg. Loss:  -0.42330045\n",
      "Epoch 0017 Policy Avg. Loss:  -0.2916847 , Critic 1 Avg. Loss:  0.2604034619955195 , Critic 2 Avg. Loss:  0.2663542634463382 , Alpha 1 Avg. Loss:  -0.38548732\n",
      "Epoch 0018 Policy Avg. Loss:  -0.29072222 , Critic 1 Avg. Loss:  0.2741701028672719 , Critic 2 Avg. Loss:  0.2588402931038094 , Alpha 1 Avg. Loss:  -0.3369592\n",
      "Epoch 0019 Policy Avg. Loss:  -0.46005228 , Critic 1 Avg. Loss:  0.24945915418259285 , Critic 2 Avg. Loss:  0.2342849354976519 , Alpha 1 Avg. Loss:  -0.2378283\n",
      "Epoch 0020 Policy Avg. Loss:  -0.41457516 , Critic 1 Avg. Loss:  0.23223061935892014 , Critic 2 Avg. Loss:  0.21331459685029028 , Alpha 1 Avg. Loss:  -0.21320784\n",
      "Epoch 0021 Policy Avg. Loss:  -0.6436346 , Critic 1 Avg. Loss:  0.23218889364639422 , Critic 2 Avg. Loss:  0.22417067687262102 , Alpha 1 Avg. Loss:  -0.1532147\n",
      "Epoch 0022 Policy Avg. Loss:  -0.9049733 , Critic 1 Avg. Loss:  0.18044496658627682 , Critic 2 Avg. Loss:  0.1735895480707774 , Alpha 1 Avg. Loss:  -0.069968656\n",
      "Epoch 0023 Policy Avg. Loss:  -1.1626871 , Critic 1 Avg. Loss:  0.16079097689695265 , Critic 2 Avg. Loss:  0.15628334573323016 , Alpha 1 Avg. Loss:  0.033253234\n",
      "Epoch 0024 Policy Avg. Loss:  -1.1985542 , Critic 1 Avg. Loss:  0.15940535311047424 , Critic 2 Avg. Loss:  0.18059076645251274 , Alpha 1 Avg. Loss:  -0.043783337\n",
      "Epoch 0025 Policy Avg. Loss:  -1.5921456 , Critic 1 Avg. Loss:  0.132223401492922 , Critic 2 Avg. Loss:  0.1455823375717586 , Alpha 1 Avg. Loss:  -0.06883473\n",
      "Epoch 0026 Policy Avg. Loss:  -1.6260238 , Critic 1 Avg. Loss:  0.1601309260893251 , Critic 2 Avg. Loss:  0.1772963682902293 , Alpha 1 Avg. Loss:  -0.018978257\n",
      "Epoch 0027 Policy Avg. Loss:  -1.8163036 , Critic 1 Avg. Loss:  0.1883077511673663 , Critic 2 Avg. Loss:  0.20476784379451426 , Alpha 1 Avg. Loss:  -0.025443908\n",
      "Epoch 0028 Policy Avg. Loss:  -2.2010984 , Critic 1 Avg. Loss:  0.37433526808932793 , Critic 2 Avg. Loss:  0.35855182757048176 , Alpha 1 Avg. Loss:  0.03475001\n",
      "Epoch 0029 Policy Avg. Loss:  -1.7778342 , Critic 1 Avg. Loss:  0.3050606991569431 , Critic 2 Avg. Loss:  0.3111679335014624 , Alpha 1 Avg. Loss:  0.0024054702\n",
      "Epoch 0030 Policy Avg. Loss:  -2.0225239 , Critic 1 Avg. Loss:  0.3028493941674583 , Critic 2 Avg. Loss:  0.31294677757054784 , Alpha 1 Avg. Loss:  0.01893696\n",
      "Epoch 0031 Policy Avg. Loss:  -2.525671 , Critic 1 Avg. Loss:  0.31545053800083095 , Critic 2 Avg. Loss:  0.3375793931515506 , Alpha 1 Avg. Loss:  0.054921474\n",
      "Epoch 0032 Policy Avg. Loss:  -2.368635 , Critic 1 Avg. Loss:  0.3385815693174496 , Critic 2 Avg. Loss:  0.41277224903633797 , Alpha 1 Avg. Loss:  0.088226244\n",
      "Epoch 0033 Policy Avg. Loss:  -2.8679335 , Critic 1 Avg. Loss:  0.38316655887300116 , Critic 2 Avg. Loss:  0.4507372364544937 , Alpha 1 Avg. Loss:  -0.0016605051\n",
      "Epoch 0034 Policy Avg. Loss:  -3.0495863 , Critic 1 Avg. Loss:  0.5233024227057623 , Critic 2 Avg. Loss:  0.563640262422366 , Alpha 1 Avg. Loss:  -0.005148149\n",
      "Epoch 0035 Policy Avg. Loss:  -3.1809163 , Critic 1 Avg. Loss:  0.5425249750678313 , Critic 2 Avg. Loss:  0.6027675945942452 , Alpha 1 Avg. Loss:  0.0032924302\n",
      "Epoch 0036 Policy Avg. Loss:  -3.5792754 , Critic 1 Avg. Loss:  0.4548241627998972 , Critic 2 Avg. Loss:  0.4925537764218633 , Alpha 1 Avg. Loss:  -0.047913853\n",
      "Epoch 0037 Policy Avg. Loss:  -3.8187623 , Critic 1 Avg. Loss:  0.3834074122150941 , Critic 2 Avg. Loss:  0.4557665201335348 , Alpha 1 Avg. Loss:  -0.077911414\n",
      "Epoch 0038 Policy Avg. Loss:  -3.854574 , Critic 1 Avg. Loss:  0.5424721550249842 , Critic 2 Avg. Loss:  0.5907723449845426 , Alpha 1 Avg. Loss:  -0.05059404\n",
      "Epoch 0039 Policy Avg. Loss:  -4.197607 , Critic 1 Avg. Loss:  0.3946646072418779 , Critic 2 Avg. Loss:  0.43104624622722854 , Alpha 1 Avg. Loss:  -0.053798925\n",
      "Epoch 0040 Policy Avg. Loss:  -4.442213 , Critic 1 Avg. Loss:  0.4580648997027182 , Critic 2 Avg. Loss:  0.49098800872202114 , Alpha 1 Avg. Loss:  -0.15693511\n",
      "Epoch 0041 Policy Avg. Loss:  -5.486438 , Critic 1 Avg. Loss:  0.533288073782583 , Critic 2 Avg. Loss:  0.5316217419772028 , Alpha 1 Avg. Loss:  -0.11044721\n",
      "Epoch 0042 Policy Avg. Loss:  -6.7760663 , Critic 1 Avg. Loss:  0.7451422186853128 , Critic 2 Avg. Loss:  0.7614333262564716 , Alpha 1 Avg. Loss:  -0.19726706\n",
      "Epoch 0043 Policy Avg. Loss:  -8.127197 , Critic 1 Avg. Loss:  0.9575861516812487 , Critic 2 Avg. Loss:  0.9736789273871933 , Alpha 1 Avg. Loss:  -0.31924665\n",
      "Epoch 0044 Policy Avg. Loss:  -9.191854 , Critic 1 Avg. Loss:  1.1593530797026326 , Critic 2 Avg. Loss:  1.1767098232713684 , Alpha 1 Avg. Loss:  -0.36018604\n",
      "Epoch 0045 Policy Avg. Loss:  -10.216724 , Critic 1 Avg. Loss:  1.460809044584683 , Critic 2 Avg. Loss:  1.4307499156861423 , Alpha 1 Avg. Loss:  -0.37315726\n",
      "Epoch 0046 Policy Avg. Loss:  -11.114316 , Critic 1 Avg. Loss:  1.7287576239213462 , Critic 2 Avg. Loss:  1.7388526626265781 , Alpha 1 Avg. Loss:  -0.4127765\n",
      "Epoch 0047 Policy Avg. Loss:  -11.650706 , Critic 1 Avg. Loss:  2.133577135440151 , Critic 2 Avg. Loss:  2.1370696816176795 , Alpha 1 Avg. Loss:  -0.59782964\n",
      "Epoch 0048 Policy Avg. Loss:  -12.115316 , Critic 1 Avg. Loss:  1.9717305803532712 , Critic 2 Avg. Loss:  2.006251371449732 , Alpha 1 Avg. Loss:  -0.56935006\n",
      "Epoch 0049 Policy Avg. Loss:  -12.69511 , Critic 1 Avg. Loss:  2.1707998278351037 , Critic 2 Avg. Loss:  2.216836526871053 , Alpha 1 Avg. Loss:  -0.68424726\n",
      "Epoch 0050 Policy Avg. Loss:  -12.939188 , Critic 1 Avg. Loss:  2.398286244947222 , Critic 2 Avg. Loss:  2.44706641552621 , Alpha 1 Avg. Loss:  -0.33372694\n",
      "Epoch 0051 Policy Avg. Loss:  -13.0788145 , Critic 1 Avg. Loss:  2.3614802927169576 , Critic 2 Avg. Loss:  2.458942755813635 , Alpha 1 Avg. Loss:  -0.3892243\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0052 Policy Avg. Loss:  -13.274349 , Critic 1 Avg. Loss:  2.2403554763609934 , Critic 2 Avg. Loss:  2.3467805706529674 , Alpha 1 Avg. Loss:  -0.42263532\n",
      "Epoch 0053 Policy Avg. Loss:  -13.674191 , Critic 1 Avg. Loss:  2.1416053612947836 , Critic 2 Avg. Loss:  2.267545965486438 , Alpha 1 Avg. Loss:  -0.38618147\n",
      "Epoch 0054 Policy Avg. Loss:  -14.439775 , Critic 1 Avg. Loss:  2.163092069731785 , Critic 2 Avg. Loss:  2.308187946550847 , Alpha 1 Avg. Loss:  -0.33284298\n",
      "Epoch 0055 Policy Avg. Loss:  -15.243756 , Critic 1 Avg. Loss:  2.5158632082963437 , Critic 2 Avg. Loss:  2.7067834075419013 , Alpha 1 Avg. Loss:  -0.22697988\n",
      "Epoch 0056 Policy Avg. Loss:  -16.58219 , Critic 1 Avg. Loss:  2.6256800947099928 , Critic 2 Avg. Loss:  2.906106890304842 , Alpha 1 Avg. Loss:  -0.2814737\n",
      "Epoch 0057 Policy Avg. Loss:  -17.1541 , Critic 1 Avg. Loss:  2.7911583702752267 , Critic 2 Avg. Loss:  3.119471925073913 , Alpha 1 Avg. Loss:  -0.30011243\n",
      "Epoch 0058 Policy Avg. Loss:  -17.315815 , Critic 1 Avg. Loss:  2.554338277986085 , Critic 2 Avg. Loss:  2.8765190833015652 , Alpha 1 Avg. Loss:  -0.26952338\n",
      "Epoch 0059 Policy Avg. Loss:  -17.59074 , Critic 1 Avg. Loss:  2.4517592353857767 , Critic 2 Avg. Loss:  2.7103202209312043 , Alpha 1 Avg. Loss:  -0.21482746\n",
      "Epoch 0060 Policy Avg. Loss:  -17.561073 , Critic 1 Avg. Loss:  2.5519947640021146 , Critic 2 Avg. Loss:  2.804514573852435 , Alpha 1 Avg. Loss:  -0.24364835\n",
      "Epoch 0061 Policy Avg. Loss:  -17.739864 , Critic 1 Avg. Loss:  2.3266165888466563 , Critic 2 Avg. Loss:  2.52381739634944 , Alpha 1 Avg. Loss:  -0.15433328\n",
      "Epoch 0062 Policy Avg. Loss:  -18.084604 , Critic 1 Avg. Loss:  2.3061226313521273 , Critic 2 Avg. Loss:  2.4833542332904703 , Alpha 1 Avg. Loss:  -0.2039504\n",
      "Epoch 0063 Policy Avg. Loss:  -18.228468 , Critic 1 Avg. Loss:  2.3703436125563146 , Critic 2 Avg. Loss:  2.579904160465725 , Alpha 1 Avg. Loss:  -0.032661047\n",
      "Epoch 0064 Policy Avg. Loss:  -18.426668 , Critic 1 Avg. Loss:  2.3315031201296788 , Critic 2 Avg. Loss:  2.5077208521290104 , Alpha 1 Avg. Loss:  -0.20833197\n",
      "Epoch 0065 Policy Avg. Loss:  -18.597727 , Critic 1 Avg. Loss:  2.3333980367844225 , Critic 2 Avg. Loss:  2.572765873254436 , Alpha 1 Avg. Loss:  -0.0011347884\n"
     ]
    }
   ],
   "source": [
    "sac1.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "94e70621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved actor weights\n",
      "Saved critic 1 weights\n",
      "Saved critic 2 weights\n",
      "Saved alpha weights\n"
     ]
    }
   ],
   "source": [
    "sac1.save_weights(\"/home/tony/rl_models/MA2C/sac/devel/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d26664a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.1228214>\n"
     ]
    }
   ],
   "source": [
    "print(sac1.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "909c1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac2 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "9ba100f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor weights\n",
      "Loaded critic 1 weights\n",
      "Loaded critic 2 weights\n",
      "Loaded alpha weights\n"
     ]
    }
   ],
   "source": [
    "sac2.load_weights(\"/home/tony/rl_models/MA2C/sac/devel/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "97bceeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.1228214>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac2.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "17c45be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0000 Policy Avg. Loss:  -21.662226 , Critic 1 Avg. Loss:  0.5611303748823558 , Critic 2 Avg. Loss:  0.8986719703315804 , Alpha 1 Avg. Loss:  -0.035006013\n",
      "Epoch 0001 Policy Avg. Loss:  -19.31563 , Critic 1 Avg. Loss:  2.4457886711160026 , Critic 2 Avg. Loss:  2.8867608816664174 , Alpha 1 Avg. Loss:  -0.042637814\n",
      "Epoch 0002 Policy Avg. Loss:  -20.108728 , Critic 1 Avg. Loss:  3.0707828047755603 , Critic 2 Avg. Loss:  3.6249288804746143 , Alpha 1 Avg. Loss:  0.10331981\n",
      "Epoch 0003 Policy Avg. Loss:  -22.031517 , Critic 1 Avg. Loss:  4.72371220025689 , Critic 2 Avg. Loss:  5.3214665246990975 , Alpha 1 Avg. Loss:  0.047583167\n",
      "Epoch 0004 Policy Avg. Loss:  -23.112915 , Critic 1 Avg. Loss:  6.409520123002741 , Critic 2 Avg. Loss:  7.095991969678641 , Alpha 1 Avg. Loss:  0.08587684\n",
      "Epoch 0005 Policy Avg. Loss:  -24.469872 , Critic 1 Avg. Loss:  7.504048047473244 , Critic 2 Avg. Loss:  8.279422929127648 , Alpha 1 Avg. Loss:  -0.107491285\n",
      "Epoch 0006 Policy Avg. Loss:  -24.586811 , Critic 1 Avg. Loss:  9.323451748204606 , Critic 2 Avg. Loss:  9.99919675003712 , Alpha 1 Avg. Loss:  -0.050815012\n",
      "Epoch 0007 Policy Avg. Loss:  -25.128695 , Critic 1 Avg. Loss:  9.679629753534998 , Critic 2 Avg. Loss:  10.255417764255819 , Alpha 1 Avg. Loss:  -0.0076999716\n",
      "Epoch 0008 Policy Avg. Loss:  -24.848331 , Critic 1 Avg. Loss:  7.870406400211873 , Critic 2 Avg. Loss:  8.299886945057203 , Alpha 1 Avg. Loss:  -0.21603182\n",
      "Epoch 0009 Policy Avg. Loss:  -24.712368 , Critic 1 Avg. Loss:  6.790498124111307 , Critic 2 Avg. Loss:  7.047316095031345 , Alpha 1 Avg. Loss:  -0.40538755\n",
      "Epoch 0010 Policy Avg. Loss:  -24.346912 , Critic 1 Avg. Loss:  7.05191508922775 , Critic 2 Avg. Loss:  7.4381736911097525 , Alpha 1 Avg. Loss:  -0.27321362\n",
      "Epoch 0011 Policy Avg. Loss:  -23.472599 , Critic 1 Avg. Loss:  5.685350999219696 , Critic 2 Avg. Loss:  5.991414885410253 , Alpha 1 Avg. Loss:  -0.1556707\n",
      "Epoch 0012 Policy Avg. Loss:  -23.053722 , Critic 1 Avg. Loss:  4.672057579932377 , Critic 2 Avg. Loss:  4.84301457396007 , Alpha 1 Avg. Loss:  -0.22976388\n",
      "Epoch 0013 Policy Avg. Loss:  -24.03502 , Critic 1 Avg. Loss:  4.271382878872714 , Critic 2 Avg. Loss:  4.52206760723149 , Alpha 1 Avg. Loss:  -0.26754665\n",
      "Epoch 0014 Policy Avg. Loss:  -26.353003 , Critic 1 Avg. Loss:  4.02048773018415 , Critic 2 Avg. Loss:  4.156488673631578 , Alpha 1 Avg. Loss:  -0.33209172\n",
      "Epoch 0015 Policy Avg. Loss:  -27.962658 , Critic 1 Avg. Loss:  5.222528077419425 , Critic 2 Avg. Loss:  5.367868647153176 , Alpha 1 Avg. Loss:  -0.24276638\n",
      "Epoch 0016 Policy Avg. Loss:  -29.244692 , Critic 1 Avg. Loss:  6.838780665296862 , Critic 2 Avg. Loss:  6.993624984359494 , Alpha 1 Avg. Loss:  -0.37495786\n",
      "Epoch 0017 Policy Avg. Loss:  -30.562195 , Critic 1 Avg. Loss:  6.624477003486683 , Critic 2 Avg. Loss:  6.838001662378709 , Alpha 1 Avg. Loss:  -0.29522067\n"
     ]
    }
   ],
   "source": [
    "sac2.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "98f816b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n",
      "rollout episodic reward:  113.71426230419229\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "113.71426230419229"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac2.eval_rollout(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e185df41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
