{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5dc7ff4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/flatbuffers/compat.py:19: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses\n",
      "  import imp\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:36: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.\n",
      "  'nearest': pil_image.NEAREST,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:37: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  'bilinear': pil_image.BILINEAR,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:38: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.\n",
      "  'bicubic': pil_image.BICUBIC,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:39: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.\n",
      "  'hamming': pil_image.HAMMING,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:40: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.\n",
      "  'box': pil_image.BOX,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/keras/utils/image_utils.py:41: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.\n",
      "  'lanczos': pil_image.LANCZOS,\n",
      "/home/tony/miniconda3/envs/rl_models/lib/python3.8/site-packages/tensorflow_probability/python/__init__.py:57: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  if (distutils.version.LooseVersion(tf.__version__) <\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import regularizers\n",
    "import glfw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8cca3c",
   "metadata": {},
   "source": [
    "### Env Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e91a9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = \"Hopper-v3\"\n",
    "env = gym.make(problem)\n",
    "\n",
    "num_states = env.observation_space.shape[0]\n",
    "num_actions = env.action_space.shape[0]\n",
    "upper_bound = env.action_space.high[0]\n",
    "lower_bound = env.action_space.low[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafe2ab",
   "metadata": {},
   "source": [
    "### Actor Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9aeb90",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279be0e3",
   "metadata": {},
   "source": [
    "Same as PPO actor. Gaussian policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e15e7f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPSILON = 1e-10\n",
    "\n",
    "class Actor(Model):\n",
    "\n",
    "    def __init__(self, action_dimensions, action_bound):\n",
    "        super().__init__()\n",
    "        self.action_dim, self.upper_bound = action_dimensions, action_bound\n",
    "        self.sample_dist = tfp.distributions.MultivariateNormalDiag(loc=tf.zeros(self.action_dim),\n",
    "                                                                    scale_diag=tf.ones(self.action_dim))\n",
    "        self.input_batch_norm = layers.BatchNormalization()\n",
    "        self.dense1_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.dense2_layer = layers.Dense(64, activation=\"relu\")\n",
    "        self.mean_layer = layers.Dense(self.action_dim)\n",
    "        self.stdev_layer = layers.Dense(self.action_dim)\n",
    "\n",
    "    def call(self, state, eval_mode=False):\n",
    "        norm_state = self.input_batch_norm(state)\n",
    "        a1 = self.dense1_layer(norm_state)\n",
    "        a2 = self.dense2_layer(a1)\n",
    "        mu = self.mean_layer(a2)\n",
    "\n",
    "        log_sigma = self.stdev_layer(a2)\n",
    "        sigma = tf.exp(log_sigma)\n",
    "        sigma = tf.clip_by_value(sigma, EPSILON, 2.718)\n",
    "\n",
    "        dist = tfp.distributions.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n",
    "        \n",
    "        if eval_mode:\n",
    "            action_ = mu\n",
    "        else:\n",
    "            action_ = tf.math.add(mu, tf.math.multiply(sigma, tf.expand_dims(self.sample_dist.sample(), 0)))\n",
    " \n",
    "        action = tf.tanh(action_)\n",
    "\n",
    "        log_pi_ = dist.log_prob(action_)     \n",
    "        log_pi = log_pi_ - tf.reduce_sum(tf.math.log(tf.clip_by_value(1 - action**2, EPSILON, 1.0)), axis=1)\n",
    "        \n",
    "        return action*self.upper_bound, log_pi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d3f74b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 10:48:09.114364: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.120669: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.120755: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.121423: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-06 10:48:09.122068: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.122156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.122229: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.406226: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.406343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.406421: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-06 10:48:09.406489: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5917 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "actor_test = Actor(num_actions, upper_bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d7cab68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.92230535 0.500152   0.6901875 ]], shape=(1, 3), dtype=float32) tf.Tensor([-2.4248033], shape=(1,), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-06 10:48:09.926555: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs\n",
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "tf_obs\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "print(a_test, log_a_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "116539f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"actor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " batch_normalization (BatchN  multiple                 44        \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  768       \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             multiple                  195       \n",
      "                                                                 \n",
      " dense_3 (Dense)             multiple                  195       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,362\n",
      "Trainable params: 5,340\n",
      "Non-trainable params: 22\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "actor_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e09fae85",
   "metadata": {},
   "source": [
    "### Critic Wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a403d18b",
   "metadata": {},
   "source": [
    "#### Note:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b347d502",
   "metadata": {},
   "source": [
    "Different from PPO, critic evaluate state-action pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5f39f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic_Wrapper():\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        self.s_dim=state_dim\n",
    "        self.a_dim=action_dim\n",
    "        \n",
    "    def get_critic(self):\n",
    "        # State as input\n",
    "        state_input = layers.Input(shape=(self.s_dim))\n",
    "        state_out = layers.Dense(32, activation=\"relu\")(state_input)\n",
    "\n",
    "        # Action as input\n",
    "        action_input = layers.Input(shape=(self.a_dim))\n",
    "        action_out = layers.Dense(32, activation=\"relu\")(action_input)\n",
    "\n",
    "        # Concatenating\n",
    "        concat = layers.Concatenate()([state_out, action_out])\n",
    "        out = layers.Dense(64, activation=\"relu\")(concat)\n",
    "        outputs = tf.squeeze(layers.Dense(1)(out))\n",
    "\n",
    "        # Outputs single value for give state-action\n",
    "        model = tf.keras.Model([state_input, action_input], outputs)\n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dadd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_gen = Critic_Wrapper(num_states, num_actions)\n",
    "critic_test = critic_gen.get_critic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65aced4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.25442901e+00, -3.80904130e-03, -5.76446639e-04,  4.18957643e-03,\n",
       "        4.75507043e-03, -3.81863356e-03, -3.10066052e-03,  3.68888497e-03,\n",
       "        1.68800979e-03,  4.59514693e-03,  2.07789733e-04])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs = env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c101830",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11), dtype=float64, numpy=\n",
       "array([[ 1.25442901e+00, -3.80904130e-03, -5.76446639e-04,\n",
       "         4.18957643e-03,  4.75507043e-03, -3.81863356e-03,\n",
       "        -3.10066052e-03,  3.68888497e-03,  1.68800979e-03,\n",
       "         4.59514693e-03,  2.07789733e-04]])>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_obs = tf.expand_dims(obs, 0)\n",
    "a_test, log_a_test = actor_test(tf_obs)\n",
    "tf_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa334304",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.43726283>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_test = critic_test([tf_obs, a_test])\n",
    "v_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a2cfe7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 11)\n"
     ]
    }
   ],
   "source": [
    "obs_new, _, _, _ = env.step(a_test[0])\n",
    "tf_obs_new = tf.expand_dims(obs_new, 0)\n",
    "statex2 = tf.convert_to_tensor([obs, obs_new])\n",
    "print(statex2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df584535",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3) (2,)\n"
     ]
    }
   ],
   "source": [
    "a_2, loga_2 = actor_test(statex2)\n",
    "\n",
    "print(a_2.shape, loga_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f16a7ea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2,)\n"
     ]
    }
   ],
   "source": [
    "v_2 = critic_test([statex2, a_2])\n",
    "print(v_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb6392ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 11)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 3)]          0           []                               \n",
      "                                                                                                  \n",
      " dense_4 (Dense)                (None, 32)           384         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " dense_5 (Dense)                (None, 32)           128         ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 64)           0           ['dense_4[0][0]',                \n",
      "                                                                  'dense_5[0][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 64)           4160        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_7 (Dense)                (None, 1)            65          ['dense_6[0][0]']                \n",
      "                                                                                                  \n",
      " tf.compat.v1.squeeze (TFOpLamb  None                0           ['dense_7[0][0]']                \n",
      " da)                                                                                              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,737\n",
      "Trainable params: 4,737\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "critic_test.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a3c971",
   "metadata": {},
   "source": [
    "### Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "66ff1c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buffer:\n",
    "    def __init__(self, obs_dim, a_dim, buffer_capacity=100000, batch_size=256):\n",
    "        \n",
    "        self.obs_dim = obs_dim\n",
    "        self.a_dim = a_dim\n",
    "        self.buffer_capacity = buffer_capacity\n",
    "        self.batch_size = batch_size\n",
    "        self.buffer_counter = 0\n",
    "\n",
    "        self.state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.action_buffer = np.zeros((self.buffer_capacity, self.a_dim))\n",
    "        self.reward_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "        self.next_state_buffer = np.zeros((self.buffer_capacity, self.obs_dim))\n",
    "        self.done_buffer = np.zeros((self.buffer_capacity, 1))\n",
    "\n",
    "    def record(self, obs_tuple):\n",
    "        index = self.buffer_counter % self.buffer_capacity\n",
    "\n",
    "        self.state_buffer[index] = obs_tuple[0]\n",
    "        self.action_buffer[index] = obs_tuple[1]\n",
    "        self.reward_buffer[index] = obs_tuple[2]\n",
    "        self.next_state_buffer[index] = obs_tuple[3]\n",
    "        self.done_buffer[index] = obs_tuple[4]\n",
    "        self.buffer_counter += 1\n",
    "        \n",
    "    def sample(self):\n",
    "        record_range = min(self.buffer_counter, self.buffer_capacity)\n",
    "        batch_indices = np.random.choice(record_range, self.batch_size)\n",
    "\n",
    "        state_batch = tf.convert_to_tensor(self.state_buffer[batch_indices])\n",
    "        action_batch = tf.convert_to_tensor(self.action_buffer[batch_indices])\n",
    "        reward_batch = tf.squeeze(tf.convert_to_tensor(self.reward_buffer[batch_indices]))\n",
    "        next_state_batch = tf.convert_to_tensor(self.next_state_buffer[batch_indices])\n",
    "        done_batch = tf.squeeze(tf.convert_to_tensor(self.done_buffer[batch_indices]))\n",
    "        \n",
    "        return (state_batch,\n",
    "               action_batch,\n",
    "               reward_batch,\n",
    "               next_state_batch,\n",
    "               done_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9d040f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer1 = Buffer(num_states, num_actions, 100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11879892",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_obs = env.reset()\n",
    "\n",
    "for i in range(buffer1.buffer_capacity):\n",
    "    a, _ = actor_test(tf.expand_dims(prev_obs, 0))\n",
    "    obs, r, d, _ = env.step(a[0])\n",
    "    \n",
    "    buffer1.record((prev_obs, a[0], r, obs, d))\n",
    "    \n",
    "    prev_obs = obs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac7dac19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 2.07513780e-01, -5.02459999e+00, -2.35363350e+00,\n",
       "         -1.34703445e+00,  1.15736795e-01, -1.63898669e+00,\n",
       "         -2.15141369e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "          1.00000000e+01,  7.26882154e+00],\n",
       "        [ 4.19387131e-01, -3.10997671e+00, -6.50439161e-01,\n",
       "         -2.66650718e+00, -4.15679480e-01,  2.02904074e+00,\n",
       "         -2.23016643e+00, -1.00000000e+01, -7.90765582e+00,\n",
       "          3.20817540e+00,  1.06045163e-01],\n",
       "        [ 5.88126089e-02, -4.66778361e+00, -2.62810207e+00,\n",
       "          4.81473720e-02,  8.29718405e-01,  4.48367461e-01,\n",
       "          4.03581660e-01, -7.70336462e-01,  3.64471820e-01,\n",
       "         -4.35945677e+00, -2.06952969e+00],\n",
       "        [ 4.02437050e-01, -3.23941011e+00, -7.18929516e-01,\n",
       "         -2.63862088e+00, -4.10764928e-01,  2.04999400e+00,\n",
       "         -2.00546025e+00, -1.00000000e+01, -9.21261458e+00,\n",
       "          3.76077355e+00,  1.12084264e+00],\n",
       "        [ 3.26118985e-01, -4.24082919e+00, -1.44660071e+00,\n",
       "         -2.24894220e+00, -2.10577307e-01, -6.64229604e-01,\n",
       "         -1.39128298e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "          1.00000000e+01,  5.71298629e+00],\n",
       "        [ 1.37792023e-01, -5.16080289e+00, -2.62021948e+00,\n",
       "         -1.23856686e+00, -4.94734228e-01,  1.34888664e+00,\n",
       "          1.40834177e+00, -7.05815403e+00,  9.13231106e-03,\n",
       "         -1.00000000e+01, -1.00000000e+01],\n",
       "        [ 2.07513780e-01, -5.02459999e+00, -2.35363350e+00,\n",
       "         -1.34703445e+00,  1.15736795e-01, -1.63898669e+00,\n",
       "         -2.15141369e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "          1.00000000e+01,  7.26882154e+00],\n",
       "        [ 1.70284404e-01, -4.77208240e+00, -2.34043291e+00,\n",
       "         -1.89695475e+00, -7.26162655e-01, -6.70846901e-01,\n",
       "         -3.20806313e-02,  5.89531954e+00,  8.20151097e-01,\n",
       "          2.12853001e+00,  6.15959933e+00],\n",
       "        [ 8.41252352e-01, -1.75013046e+00, -3.45977374e-01,\n",
       "         -1.98928451e+00, -3.78209850e-01,  3.54956792e-01,\n",
       "         -5.54346559e+00, -1.00000000e+01, -2.79847245e+00,\n",
       "         -1.00000000e+01, -1.06679400e+00],\n",
       "        [ 1.79383665e-01, -5.10510518e+00, -2.43646585e+00,\n",
       "         -1.91132515e+00, -8.95536951e-01,  6.33450894e-01,\n",
       "         -7.42948300e-01,  1.00000000e+01,  8.11146088e+00,\n",
       "         -2.55259883e+00,  3.06088183e+00]])>,\n",
       " <tf.Tensor: shape=(10, 3), dtype=float64, numpy=\n",
       " array([[-0.8510347 ,  0.28501278, -0.66051543],\n",
       "        [-0.8981455 ,  0.44786698,  0.71349394],\n",
       "        [-0.77536243, -0.53313267, -0.8835063 ],\n",
       "        [-0.8786661 ,  0.98987722,  0.22439055],\n",
       "        [-0.90450794,  0.2436206 ,  0.24780971],\n",
       "        [-0.53188664, -0.95378339, -0.64329821],\n",
       "        [-0.8510347 ,  0.28501278, -0.66051543],\n",
       "        [-0.03639552, -0.99637371, -0.39549181],\n",
       "        [-0.10742237, -0.99862373,  0.54297858],\n",
       "        [-0.02515618,  0.08076844,  0.01926509]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=\n",
       " array([-0.70609313,  3.04106997,  1.46646128,  2.98170567,  0.04154854,\n",
       "         2.52010078, -0.70609313,  0.41108302,  1.60921094,  1.25790505])>,\n",
       " <tf.Tensor: shape=(10, 11), dtype=float64, numpy=\n",
       " array([[ 1.90137495e-01, -5.15473236e+00, -2.51340289e+00,\n",
       "         -1.21095736e+00,  1.69473631e-01, -1.77371470e+00,\n",
       "         -2.19725379e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "          1.00000000e+01,  6.16684097e+00],\n",
       "        [ 4.02437050e-01, -3.23941011e+00, -7.18929516e-01,\n",
       "         -2.63862088e+00, -4.10764928e-01,  2.04999400e+00,\n",
       "         -2.00546025e+00, -1.00000000e+01, -9.21261458e+00,\n",
       "          3.76077355e+00,  1.12084264e+00],\n",
       "        [ 6.17389577e-02, -4.67622073e+00, -2.62569799e+00,\n",
       "          1.04631841e-02,  8.08160589e-01,  4.88233794e-01,\n",
       "          3.27401615e-01, -1.33149206e+00,  2.44504194e-01,\n",
       "         -5.06077370e+00, -3.31861724e+00],\n",
       "        [ 3.87442381e-01, -3.37401109e+00, -7.98028526e-01,\n",
       "         -2.60432015e+00, -4.00802371e-01,  1.90855969e+00,\n",
       "         -1.74580033e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "          4.81132119e+00,  1.36879082e+00],\n",
       "        [ 3.13435108e-01, -4.36448806e+00, -1.56726825e+00,\n",
       "         -2.13974194e+00, -1.66701500e-01, -1.23726839e+00,\n",
       "         -1.79629054e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "          1.00000000e+01,  5.26759801e+00],\n",
       "        [ 1.49191758e-01, -5.22046143e+00, -2.62015310e+00,\n",
       "         -1.36720510e+00, -6.43731791e-01,  1.69678058e+00,\n",
       "          1.44112836e+00, -7.85102466e+00,  7.30691396e-03,\n",
       "         -1.00000000e+01, -1.00000000e+01],\n",
       "        [ 1.90137495e-01, -5.15473236e+00, -2.51340289e+00,\n",
       "         -1.21095736e+00,  1.69473631e-01, -1.77371470e+00,\n",
       "         -2.19725379e+00, -1.00000000e+01, -1.00000000e+01,\n",
       "          1.00000000e+01,  6.16684097e+00],\n",
       "        [ 1.69943352e-01, -4.73271008e+00, -2.33438571e+00,\n",
       "         -1.88347774e+00, -6.77173750e-01, -5.09066298e-01,\n",
       "         -6.49664249e-02,  4.08568260e+00,  7.31685894e-01,\n",
       "          1.27862984e+00,  6.05351585e+00],\n",
       "        [ 7.95415013e-01, -1.86451842e+00, -3.67699244e-01,\n",
       "         -2.09096198e+00, -3.83467761e-01,  8.77583838e-01,\n",
       "         -5.90856897e+00, -1.00000000e+01, -2.62898164e+00,\n",
       "         -1.00000000e+01, -2.48192492e-01],\n",
       "        [ 1.74783722e-01, -4.98827019e+00, -2.38825480e+00,\n",
       "         -1.92228138e+00, -8.64799672e-01, -4.15525261e-02,\n",
       "         -4.26981715e-01,  1.00000000e+01,  4.30008959e+00,\n",
       "         -3.70304765e-01,  4.50055838e+00]])>,\n",
       " <tf.Tensor: shape=(10,), dtype=float64, numpy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])>)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buffer1.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956d47d",
   "metadata": {},
   "source": [
    "### Soft Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7377d1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    \n",
    "    def __init__(self, env, observation_dimensions, action_dimensions, action_bound, buffer_capacity,\n",
    "                 minibatch_size=256, gamma=0.99, tau=0.95, lr=3e-4):\n",
    "        \n",
    "        self.env = env\n",
    "        \n",
    "        self.a = Actor(action_dimensions, action_bound)\n",
    "        self.c_gen = Critic_Wrapper(observation_dimensions, action_dimensions)\n",
    "        self.c1 = self.c_gen.get_critic()\n",
    "        self.c2 = self.c_gen.get_critic()\n",
    "        self.tc1 = self.c_gen.get_critic()\n",
    "        self.tc2 = self.c_gen.get_critic()\n",
    "        \n",
    "        self.tc1.set_weights(self.c1.get_weights())\n",
    "        self.tc2.set_weights(self.c2.get_weights())\n",
    "\n",
    "        self.te = -np.prod(action_dimensions)\n",
    "        self.alpha = tf.Variable(0.0, dtype=tf.float32)\n",
    "        \n",
    "        self.a_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c1_opt = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        self.c2_opt = tf.keras.optimizers.Adam(learning_rate=lr)                                                  \n",
    "        self.alpha_opt = tf.keras.optimizers.Adam(learning_rate=lr)   \n",
    "        \n",
    "        self.buffer = Buffer(observation_dimensions, action_dimensions, buffer_capacity, minibatch_size)\n",
    "        \n",
    "        self.gamma, self.tau = gamma, tau\n",
    "        \n",
    "    def train(self, max_env_step):\n",
    "        t = 0\n",
    "        a_losses = []\n",
    "        c1_losses = []\n",
    "        c2_losses = []\n",
    "        alpha_losses = []\n",
    "        while t < max_env_step:\n",
    "            p_s = self.env.reset()\n",
    "\n",
    "            while True:\n",
    "                a, log_a = self.a(tf.expand_dims(p_s, 0))\n",
    "                a=a[0]\n",
    "                s, r, d, _ = self.env.step(a)\n",
    "                end = 0 if d else 1\n",
    "                \n",
    "                self.buffer.record((p_s, a, r, s, end))\n",
    "                data = self.buffer.sample()\n",
    "                \n",
    "                a_loss, c1_loss, c2_loss, alpha_loss = self.update(data)\n",
    "                \n",
    "                a_losses.append(a_loss.numpy())\n",
    "                c1_losses.append(c1_loss.numpy())\n",
    "                c2_losses.append(c2_loss.numpy())\n",
    "                alpha_losses.append(alpha_loss.numpy())\n",
    "                \n",
    "                t = t+1\n",
    "                \n",
    "                if d:\n",
    "                    break\n",
    "                p_s = s\n",
    "                \n",
    "        print(\"Per {:04d} Steps\".format(max_env_step), \"Policy Avg. Loss: \", np.mean(a_losses), \n",
    "              \", Critic 1 Avg. Loss: \",  np.mean(c1_losses), \n",
    "              \", Critic 2 Avg. Loss: \",  np.mean(c2_losses), \n",
    "              \", Alpha 1 Avg. Loss: \",  np.mean(alpha_losses), flush=True)\n",
    "\n",
    "\n",
    "    @tf.function\n",
    "    def update(self, data):\n",
    "        s_b, a_b, r_b, ns_b, d_b = data\n",
    "        with tf.GradientTape() as tape_c1, tf.GradientTape() as tape_c2:\n",
    "            q1 = self.c1([s_b, a_b])\n",
    "            q2 = self.c2([s_b, a_b])\n",
    "            na, nlog_a = self.a(ns_b, training=True)\n",
    "            \n",
    "            tq1 = self.tc1([ns_b, na])\n",
    "            tq2 = self.tc2([ns_b, na])\n",
    "            \n",
    "            min_qt = tf.math.minimum(tq1,tq2)\n",
    "            \n",
    "            soft_qt = min_qt - (self.alpha*nlog_a)\n",
    "            \n",
    "            y = tf.stop_gradient(r_b+self.gamma*d_b*tf.cast(soft_qt, dtype=tf.float64))\n",
    "            \n",
    "            L_c1 = 0.5*tf.reduce_mean((y-tf.cast(q1, dtype=tf.float64))**2)\n",
    "            L_c2 = 0.5*tf.reduce_mean((y-tf.cast(q2, dtype=tf.float64))**2)\n",
    "        c1_grad = tape_c1.gradient(L_c1, self.c1.trainable_variables)\n",
    "        c2_grad = tape_c2.gradient(L_c2, self.c2.trainable_variables)\n",
    "        \n",
    "        self.c1_opt.apply_gradients(zip(c1_grad, self.c1.trainable_variables))\n",
    "        self.c2_opt.apply_gradients(zip(c2_grad, self.c2.trainable_variables))\n",
    "        \n",
    "        for (tc1w, c1w) in zip(self.tc1.variables, self.c1.variables):\n",
    "            tc1w.assign(tc1w*self.tau + c1w*(1.0-self.tau))\n",
    "        for (tc2w, c2w) in zip(self.tc2.variables, self.c2.variables):\n",
    "            tc2w.assign(tc2w*self.tau + c2w*(1.0-self.tau))\n",
    "            \n",
    "        with tf.GradientTape() as tape_a, tf.GradientTape() as tape_alpha:\n",
    "            a, log_a = self.a(s_b, training=True)\n",
    "            qa1 = self.c1([s_b, a])\n",
    "            qa2 = self.c2([s_b, a])\n",
    "            \n",
    "            soft_qa = tf.math.minimum(qa1,qa2)\n",
    "\n",
    "            L_a = -tf.reduce_mean(soft_qa-self.alpha*log_a)\n",
    "            L_alpha = -tf.reduce_mean(self.alpha*tf.stop_gradient(log_a + self.te))\n",
    "        grad_a = tape_a.gradient(L_a, self.a.trainable_variables)\n",
    "        grad_alpha = tape_alpha.gradient(L_alpha, [self.alpha])\n",
    "        self.a_opt.apply_gradients(zip(grad_a, self.a.trainable_variables))\n",
    "        self.alpha_opt.apply_gradients(zip(grad_alpha, [self.alpha]))\n",
    "        \n",
    "        return L_a, L_c1, L_c2, L_alpha\n",
    "    \n",
    "    def save_weights(self, dir_path):\n",
    "        cp = tf.train.Checkpoint(step=self.alpha)\n",
    "        self.a.save_weights(dir_path+\"/a.ckpt\")\n",
    "        print(\"Saved actor weights\", flush=True)\n",
    "        self.c1.save_weights(dir_path+\"/c1.ckpt\")\n",
    "        print(\"Saved critic 1 weights\", flush=True)\n",
    "        self.c2.save_weights(dir_path+\"/c2.ckpt\")\n",
    "        print(\"Saved critic 2 weights\", flush=True)\n",
    "        cp.save(dir_path+\"/alpha\")\n",
    "        print(\"Saved alpha weights\", flush=True)\n",
    "\n",
    "    def load_weights(self, dir_path):\n",
    "        try:\n",
    "            cp = tf.train.Checkpoint(step=self.alpha)\n",
    "            self.a.load_weights(dir_path+\"/a.ckpt\")\n",
    "            print(\"Loaded actor weights\", flush=True)\n",
    "            self.c1.load_weights(dir_path+\"/c1.ckpt\")\n",
    "            print(\"Loaded critic 1 weights\", flush=True)\n",
    "            self.c2.load_weights(dir_path+\"/c2.ckpt\")\n",
    "            print(\"Loaded critic 2 weights\", flush=True)\n",
    "            cp.restore(dir_path+\"/alpha-1\")\n",
    "            print(\"Loaded alpha weights\", flush=True)\n",
    "            self.tc1.set_weights(self.c1.get_weights())\n",
    "            self.tc2.set_weights(self.c2.get_weights())\n",
    "\n",
    "        except ValueError:\n",
    "            print(\"ERROR: Please make sure weights are saved as .ckpt\", flush=True)\n",
    "            \n",
    "    def eval_rollout(self, problem, rbs=False, render=False):\n",
    "        eps_r = 0\n",
    "        \n",
    "        if rbs:\n",
    "            domain, task, controller = problem\n",
    "            eval_env = Robosuite_Wrapper(domain, task, controller)\n",
    "        else:\n",
    "            eval_env = gym.make(problem)\n",
    "            \n",
    "        eval_obs = eval_env.reset()\n",
    "\n",
    "        while True:\n",
    "            if render:\n",
    "                eval_env.render()\n",
    "\n",
    "            tf_eval_obs = tf.expand_dims(tf.convert_to_tensor(eval_obs), 0)\n",
    "\n",
    "            eval_a, eval_log_a = self.a(tf_eval_obs, eval_mode=True)\n",
    "\n",
    "            eval_a = eval_a[0]\n",
    "\n",
    "            eval_obs_new, eval_r, eval_d, _ = eval_env.step(eval_a)\n",
    "\n",
    "            eps_r += eval_r\n",
    "\n",
    "            if eval_d:\n",
    "                break\n",
    "                \n",
    "            eval_obs = eval_obs_new\n",
    "        \n",
    "        if render:\n",
    "            glfw.destroy_window(eval_env.viewer.window)\n",
    "        eval_env.close()\n",
    "        print(\"rollout episodic reward: \", eps_r, flush=True)\n",
    "        \n",
    "        return eps_r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "75bddba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac1 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0ba42cdb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per 1000 Steps Policy Avg. Loss:  -30.920944 , Critic 1 Avg. Loss:  7.373537657530021 , Critic 2 Avg. Loss:  7.359247447815564 , Alpha 1 Avg. Loss:  -0.078827545\n"
     ]
    }
   ],
   "source": [
    "sac1.train(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94e70621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved actor weights\n",
      "Saved critic 1 weights\n",
      "Saved critic 2 weights\n",
      "Saved alpha weights\n"
     ]
    }
   ],
   "source": [
    "sac1.save_weights(\"/home/tony/rl_models/MA2C/sac/devel/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d26664a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.17972916>\n"
     ]
    }
   ],
   "source": [
    "print(sac1.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "909c1747",
   "metadata": {},
   "outputs": [],
   "source": [
    "sac2 = SAC(env, num_states, num_actions, upper_bound, 1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ba100f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded actor weights\n",
      "Loaded critic 1 weights\n",
      "Loaded critic 2 weights\n",
      "Loaded alpha weights\n"
     ]
    }
   ],
   "source": [
    "sac2.load_weights(\"/home/tony/rl_models/MA2C/sac/devel/weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "97bceeb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.17972916>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac2.alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "17c45be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per 0500 Steps Policy Avg. Loss:  -44.68666 , Critic 1 Avg. Loss:  2.312348485888023 , Critic 2 Avg. Loss:  2.176715956857463 , Alpha 1 Avg. Loss:  0.018258436\n"
     ]
    }
   ],
   "source": [
    "sac2.train(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "98f816b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rollout episodic reward:  245.9847486148703\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "245.9847486148703"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sac2.eval_rollout(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8a8749",
   "metadata": {},
   "source": [
    "### Robosuite Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e185df41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import robosuite as suite\n",
    "from gym import spaces\n",
    "from robosuite import load_controller_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7f2c0a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Robosuite_Wrapper():\n",
    "\n",
    "    def __init__(self, domain, task, controller, render=False):\n",
    "        self.config = load_controller_config(default_controller=controller)\n",
    "        self.env = suite.make(env_name=task, # try with other tasks like \"Stack\" and \"Door\"\n",
    "                            robots=domain,  # try with other robots like \"Sawyer\" and \"Jaco\"\n",
    "                            controller_configs=self.config,\n",
    "                            has_renderer=render,\n",
    "                            ignore_done=False,\n",
    "                            has_offscreen_renderer=False,\n",
    "                            use_camera_obs=False,\n",
    "                            reward_shaping=True,\n",
    "                            )\n",
    "        self.obs_keys = [key for key, value in self.env.observation_spec().items()]\n",
    "        \n",
    "        obs_dim = []\n",
    "        \n",
    "        for x in self.obs_keys:\n",
    "            if x == 'hinge_qpos' or x == 'handle_qpos':\n",
    "                obs_dim.append(1)\n",
    "            else:\n",
    "                obs_dim.append(self.env.observation_spec()[x].shape[0])\n",
    "                \n",
    "        self.s_dim = int(np.sum(obs_dim,dtype=np.int32))\n",
    "        self.a_dim = self.env.action_dim\n",
    "        self.a_ub = self.env.action_spec[1][0]\n",
    "        self.a_lb = self.env.action_spec[0][0]\n",
    "        \n",
    "    def env_specs(self):\n",
    "        return self.s_dim, self.a_dim, self.a_ub, self.a_lb\n",
    "    \n",
    "    def step(self, a):\n",
    "        s, r, d, i = self.env.step(a)\n",
    "\n",
    "        s = np.concatenate([s[x] for x in self.obs_keys], axis = None)\n",
    "\n",
    "        return s, r, d, i\n",
    "    \n",
    "    def reset(self):\n",
    "        s = self.env.reset()\n",
    "        \n",
    "        s = np.concatenate([s[x] for x in self.obs_keys], axis = None)\n",
    "        \n",
    "        return s\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "    \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bdec9614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating window glfw\n"
     ]
    }
   ],
   "source": [
    "rbs_env = Robosuite_Wrapper(\"Sawyer\", \"Door\", \"JOINT_VELOCITY\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f6a8fff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(92, 8, 1.0, -1.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbs_env.env_specs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f1e92d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 9.99663108e-01,  3.83852580e-01,  9.99990507e-01, -5.73705132e-01,\n",
       "         9.99931603e-01,  8.36963452e-01,  1.64081601e-02,  2.59551665e-02,\n",
       "        -9.23394388e-01, -4.35720701e-03,  8.19061916e-01, -1.16957108e-02,\n",
       "         5.47258787e-01, -9.99865377e-01,  2.40518402e-02,  5.12743937e-02,\n",
       "         3.16966832e-02,  4.32120501e-02,  2.10500630e-02,  1.23993573e-02,\n",
       "         2.56394888e-02, -1.19866428e-01,  1.68345454e-01,  1.01697855e+00,\n",
       "         9.95468587e-01,  9.47804286e-02, -6.79649258e-03,  3.57346530e-03,\n",
       "         2.07727386e-02, -2.07696841e-02, -3.01306876e-02,  3.16579302e-02,\n",
       "        -1.93281696e-01, -3.50692008e-01,  1.10000000e+00, -1.41187976e-01,\n",
       "        -2.51766559e-01,  1.07500000e+00, -7.34152677e-02, -5.19037462e-01,\n",
       "         8.30214529e-02, -2.13215481e-02, -4.20112013e-01,  5.80214529e-02,\n",
       "         2.14833941e-12, -3.95547419e-07,  9.99663108e-01,  3.83852580e-01,\n",
       "         9.99990507e-01, -5.73705132e-01,  9.99931603e-01,  8.36963452e-01,\n",
       "         1.64081601e-02,  2.59551665e-02, -9.23394388e-01, -4.35720701e-03,\n",
       "         8.19061916e-01, -1.16957108e-02,  5.47258787e-01, -9.99865377e-01,\n",
       "         2.40518402e-02,  5.12743937e-02,  3.16966832e-02,  4.32120501e-02,\n",
       "         2.10500630e-02,  1.23993573e-02,  2.56394888e-02, -1.19866428e-01,\n",
       "         1.68345454e-01,  1.01697855e+00,  9.95468587e-01,  9.47804286e-02,\n",
       "        -6.79649258e-03,  3.57346530e-03,  2.07727386e-02, -2.07696841e-02,\n",
       "        -3.01306876e-02,  3.16579302e-02, -1.93281696e-01, -3.50692008e-01,\n",
       "         1.10000000e+00, -1.41187976e-01, -2.51766559e-01,  1.07500000e+00,\n",
       "        -7.34152677e-02, -5.19037462e-01,  8.30214529e-02, -2.13215481e-02,\n",
       "        -4.20112013e-01,  5.80214529e-02,  2.14833941e-12, -3.95547419e-07]),\n",
       " 8.434504393062528e-05,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbs_env.step(np.ones(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9951f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "rbs_env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac7d66e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
